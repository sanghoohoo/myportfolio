[
  {
    "objectID": "posts/houseprice/index.html",
    "href": "posts/houseprice/index.html",
    "title": "Kaggle: Houseprice -stacking",
    "section": "",
    "text": "House Prices - Advanced Regression Techniques"
  },
  {
    "objectID": "posts/houseprice/index.html#kaggle-링크",
    "href": "posts/houseprice/index.html#kaggle-링크",
    "title": "Kaggle: Houseprice -stacking",
    "section": "",
    "text": "House Prices - Advanced Regression Techniques"
  },
  {
    "objectID": "posts/houseprice/index.html#전처리",
    "href": "posts/houseprice/index.html#전처리",
    "title": "Kaggle: Houseprice -stacking",
    "section": "전처리",
    "text": "전처리\n\n# 필요한 패키지 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(20240911) \n\n## 필요한 데이터 불러오기\nhouse_train=pd.read_csv(\"train.csv\")\nhouse_test=pd.read_csv(\"test.csv\")\nsub_df=pd.read_csv(\"sample_submission.csv\")\n\n## NaN 채우기\n# 각 숫치형 변수는 평균 채우기\n# 각 범주형 변수는 Unknown 채우기\nhouse_train.isna().sum()\nhouse_test.isna().sum()\n\n## 숫자형 채우기\nquantitative = house_train.select_dtypes(include = [int, float])\nquantitative.isna().sum()\nquant_selected = quantitative.columns[quantitative.isna().sum() &gt; 0]\n\nfor col in quant_selected:\n    house_train[col] = house_train[col].fillna(house_train[col].mean())\nhouse_train[quant_selected].isna().sum()\n\n## 범주형 채우기\nqualitative = house_train.select_dtypes(include = [object])\nqualitative.isna().sum()\nqual_selected = qualitative.columns[qualitative.isna().sum() &gt; 0]\n\nfor col in qual_selected:\n    house_train[col] = house_train[col].fillna(\"unknown\")\n\n\n# test 데이터 채우기\n## 숫자형 채우기\nquantitative = house_test.select_dtypes(include = [int, float])\nquantitative.isna().sum()\nquant_selected = quantitative.columns[quantitative.isna().sum() &gt; 0]\n\nfor col in quant_selected:\n    house_test[col] = house_test[col].fillna(house_train[col].mean())\nhouse_test[quant_selected].isna().sum()\n\n## 범주형 채우기\nqualitative = house_test.select_dtypes(include = [object])\nqualitative.isna().sum()\nqual_selected = qualitative.columns[qualitative.isna().sum() &gt; 0]\n\nfor col in qual_selected:\n    house_test[col] = house_test[col].fillna(\"unknown\")\nhouse_test[qual_selected].isna().sum()\n\n\nhouse_train.shape\nhouse_test.shape\ntrain_n=len(house_train)\n\n# 통합 df 만들기 + 더미코딩\n# house_test.select_dtypes(include=[int, float])\n\ndf = pd.concat([house_train, house_test], ignore_index=True)\n# df.info()\ndf = pd.get_dummies(\n    df,\n    columns= df.select_dtypes(include=[object]).columns,\n    drop_first=True\n    )\ndf\n\n# train / test 데이터셋\ntrain_df=df.iloc[:train_n,]\ntest_df=df.iloc[train_n:,]\n\n## 이상치 탐색\ntrain_df=train_df.query(\"GrLivArea &lt;= 4500\")\n\n## train\ntrain_x=train_df.drop(\"SalePrice\", axis=1)\ntrain_y=train_df[\"SalePrice\"]\n\n## test\ntest_x=test_df.drop(\"SalePrice\", axis=1)\n\n# 표준화\nfrom sklearn.preprocessing import StandardScaler\nnum_features = house_test.select_dtypes(include = [int, float]).columns\n\nscaler = StandardScaler()\ntrain_x[num_features] = scaler.fit_transform(train_x[num_features])\ntest_x[num_features] = scaler.transform(test_x[num_features])"
  },
  {
    "objectID": "posts/houseprice/index.html#모델-생성-및-예측",
    "href": "posts/houseprice/index.html#모델-생성-및-예측",
    "title": "Kaggle: Houseprice -stacking",
    "section": "모델 생성 및 예측",
    "text": "모델 생성 및 예측\n\n# 부스트 모델 생성\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nxgb_model = xgb.XGBRegressor(random_state=20240911)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',\n    cv=5\n)\n\ngrid_search.fit(train_x, train_y)\n\nbest_params = grid_search.best_params_\nbest_xgb_model = grid_search.best_estimator_\n\n# rf 모델 생성\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(random_state=20240911, n_estimators=100, max_features=None)\n\nparam_grid={\n    'max_depth': [25],\n    'min_samples_split': [3]\n}\n\ngrid_search=GridSearchCV(\n    estimator=model,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',\n    cv=5\n)\n\ngrid_search.fit(train_x,train_y)\ngrid_search.best_params_\nbest_rf_model=grid_search.best_estimator_\n\n# 스택킹\ny1_hat=best_xgb_model.predict(train_x)\ny2_hat=best_rf_model.predict(train_x)\n\ntrain_x_stack=pd.DataFrame({\n    'y1':y1_hat,\n    'y2':y2_hat\n})\n\npred_y_xgb=best_xgb_model.predict(test_x)\npred_y_rf=best_rf_model.predict(test_x)\n\ntest_x_stack=pd.DataFrame({\n    'y1': pred_y_xgb,\n    'y2': pred_y_rf\n})\n\n# 블렌더\nfrom sklearn.linear_model import LinearRegression\nblender_model = LinearRegression()\nblender_model.fit(train_x_stack, train_y)\npred_y = blender_model.predict(test_x_stack)\n\n# SalePrice 바꿔치기\nsub_df[\"SalePrice\"] = pred_y\nsub_df\n\n# # csv 파일로 내보내기\nsub_df.to_csv(\"sample_submission_boost_rf.csv\", index=False)"
  },
  {
    "objectID": "posts/houseprice/index.html#모델-성능",
    "href": "posts/houseprice/index.html#모델-성능",
    "title": "Kaggle: Houseprice -stacking",
    "section": "모델 성능",
    "text": "모델 성능\n\n# 모델 성능 평가\nfrom sklearn.metrics import mean_squared_error\ntrain_y_pred = blender_model.predict(train_x_stack)\nmse = mean_squared_error(train_y, train_y_pred)\nprint(f\"Mean Squared Error on training set: {mse:.2f}\")\n\nMean Squared Error on training set: 63552590.01\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/yolo8/index.html",
    "href": "posts/yolo8/index.html",
    "title": "YOLO8",
    "section": "",
    "text": "from ultralytics import YOLO\nimport matplotlib.pyplot as plt\n\n# 모델 불러오기 \nmodel = YOLO('yolov8n.pt')\n\n# 이미지에 대한 객체 감지 수행\nresults = model('pb.jpg')\n\n# 결과 시각화\nfor result in results:\n    result_img = result.plot()\n\n    # 이미지 배열을 Matplotlib을 통해 표시\n    plt.figure(figsize=(10, 10))\n    plt.imshow(result_img)\n    plt.axis('off')  # 축 숨기기\n    plt.show()\n\n\nimage 1/1 C:\\Users\\USER\\Documents\\LS bigdataschool\\myportfolio\\posts\\yolo8\\pb.jpg: 640x480 12 persons, 1 bicycle, 2 dogs, 1 cow, 1 handbag, 194.0ms\nSpeed: 6.7ms preprocess, 194.0ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 480)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw7.1/index.html",
    "href": "posts/hw7.1/index.html",
    "title": "HW7.1",
    "section": "",
    "text": "데이터를 로드하고, 로지스틱 회귀모델을 적합하고, 회귀 표를 작성하세요.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../../data/leukemia_remission.txt', sep='\\t')\ndf\n\n\n\n\n\n\n\n\nREMISS\nCELL\nSMEAR\nINFIL\nLI\nBLAST\nTEMP\n\n\n\n\n0\n1\n0.80\n0.83\n0.66\n1.9\n1.10\n1.00\n\n\n1\n1\n0.90\n0.36\n0.32\n1.4\n0.74\n0.99\n\n\n2\n0\n0.80\n0.88\n0.70\n0.8\n0.18\n0.98\n\n\n3\n0\n1.00\n0.87\n0.87\n0.7\n1.05\n0.99\n\n\n4\n1\n0.90\n0.75\n0.68\n1.3\n0.52\n0.98\n\n\n5\n0\n1.00\n0.65\n0.65\n0.6\n0.52\n0.98\n\n\n6\n1\n0.95\n0.97\n0.92\n1.0\n1.23\n0.99\n\n\n7\n0\n0.95\n0.87\n0.83\n1.9\n1.35\n1.02\n\n\n8\n0\n1.00\n0.45\n0.45\n0.8\n0.32\n1.00\n\n\n9\n0\n0.95\n0.36\n0.34\n0.5\n0.00\n1.04\n\n\n10\n0\n0.85\n0.39\n0.33\n0.7\n0.28\n0.99\n\n\n11\n0\n0.70\n0.76\n0.53\n1.2\n0.15\n0.98\n\n\n12\n0\n0.80\n0.46\n0.37\n0.4\n0.38\n1.01\n\n\n13\n0\n0.20\n0.39\n0.08\n0.8\n0.11\n0.99\n\n\n14\n0\n1.00\n0.90\n0.90\n1.1\n1.04\n0.99\n\n\n15\n1\n1.00\n0.84\n0.84\n1.9\n2.06\n1.02\n\n\n16\n0\n0.65\n0.42\n0.27\n0.5\n0.11\n1.01\n\n\n17\n0\n1.00\n0.75\n0.75\n1.0\n1.32\n1.00\n\n\n18\n0\n0.50\n0.44\n0.22\n0.6\n0.11\n0.99\n\n\n19\n1\n1.00\n0.63\n0.63\n1.1\n1.07\n0.99\n\n\n20\n0\n1.00\n0.33\n0.33\n0.4\n0.18\n1.01\n\n\n21\n0\n0.90\n0.93\n0.84\n0.6\n1.59\n1.02\n\n\n22\n1\n1.00\n0.58\n0.58\n1.0\n0.53\n1.00\n\n\n23\n0\n0.95\n0.32\n0.30\n1.6\n0.89\n0.99\n\n\n24\n1\n1.00\n0.60\n0.60\n1.7\n0.96\n0.99\n\n\n25\n1\n1.00\n0.69\n0.69\n0.9\n0.40\n0.99\n\n\n26\n0\n1.00\n0.73\n0.73\n0.7\n0.40\n0.99\n\n\n\n\n\n\n\n\nmodel = sm.formula.logit(\"REMISS ~ CELL + SMEAR + INFIL + LI + BLAST + TEMP\", data=df).fit()\n\nprint(model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.399886\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 REMISS   No. Observations:                   27\nModel:                          Logit   Df Residuals:                       20\nMethod:                           MLE   Df Model:                            6\nDate:                Fri, 27 Sep 2024   Pseudo R-squ.:                  0.3718\nTime:                        16:24:16   Log-Likelihood:                -10.797\nconverged:                       True   LL-Null:                       -17.186\nCovariance Type:            nonrobust   LLR p-value:                   0.04670\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     64.2581     74.965      0.857      0.391     -82.670     211.187\nCELL          30.8301     52.135      0.591      0.554     -71.353     133.013\nSMEAR         24.6863     61.526      0.401      0.688     -95.903     145.275\nINFIL        -24.9745     65.281     -0.383      0.702    -152.923     102.974\nLI             4.3605      2.658      1.641      0.101      -0.849       9.570\nBLAST         -0.0115      2.266     -0.005      0.996      -4.453       4.430\nTEMP        -100.1734     77.753     -1.288      0.198    -252.567      52.220\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.11 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified."
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-1.",
    "href": "posts/hw7.1/index.html#문제-1.",
    "title": "HW7.1",
    "section": "",
    "text": "데이터를 로드하고, 로지스틱 회귀모델을 적합하고, 회귀 표를 작성하세요.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv('../../data/leukemia_remission.txt', sep='\\t')\ndf\n\n\n\n\n\n\n\n\nREMISS\nCELL\nSMEAR\nINFIL\nLI\nBLAST\nTEMP\n\n\n\n\n0\n1\n0.80\n0.83\n0.66\n1.9\n1.10\n1.00\n\n\n1\n1\n0.90\n0.36\n0.32\n1.4\n0.74\n0.99\n\n\n2\n0\n0.80\n0.88\n0.70\n0.8\n0.18\n0.98\n\n\n3\n0\n1.00\n0.87\n0.87\n0.7\n1.05\n0.99\n\n\n4\n1\n0.90\n0.75\n0.68\n1.3\n0.52\n0.98\n\n\n5\n0\n1.00\n0.65\n0.65\n0.6\n0.52\n0.98\n\n\n6\n1\n0.95\n0.97\n0.92\n1.0\n1.23\n0.99\n\n\n7\n0\n0.95\n0.87\n0.83\n1.9\n1.35\n1.02\n\n\n8\n0\n1.00\n0.45\n0.45\n0.8\n0.32\n1.00\n\n\n9\n0\n0.95\n0.36\n0.34\n0.5\n0.00\n1.04\n\n\n10\n0\n0.85\n0.39\n0.33\n0.7\n0.28\n0.99\n\n\n11\n0\n0.70\n0.76\n0.53\n1.2\n0.15\n0.98\n\n\n12\n0\n0.80\n0.46\n0.37\n0.4\n0.38\n1.01\n\n\n13\n0\n0.20\n0.39\n0.08\n0.8\n0.11\n0.99\n\n\n14\n0\n1.00\n0.90\n0.90\n1.1\n1.04\n0.99\n\n\n15\n1\n1.00\n0.84\n0.84\n1.9\n2.06\n1.02\n\n\n16\n0\n0.65\n0.42\n0.27\n0.5\n0.11\n1.01\n\n\n17\n0\n1.00\n0.75\n0.75\n1.0\n1.32\n1.00\n\n\n18\n0\n0.50\n0.44\n0.22\n0.6\n0.11\n0.99\n\n\n19\n1\n1.00\n0.63\n0.63\n1.1\n1.07\n0.99\n\n\n20\n0\n1.00\n0.33\n0.33\n0.4\n0.18\n1.01\n\n\n21\n0\n0.90\n0.93\n0.84\n0.6\n1.59\n1.02\n\n\n22\n1\n1.00\n0.58\n0.58\n1.0\n0.53\n1.00\n\n\n23\n0\n0.95\n0.32\n0.30\n1.6\n0.89\n0.99\n\n\n24\n1\n1.00\n0.60\n0.60\n1.7\n0.96\n0.99\n\n\n25\n1\n1.00\n0.69\n0.69\n0.9\n0.40\n0.99\n\n\n26\n0\n1.00\n0.73\n0.73\n0.7\n0.40\n0.99\n\n\n\n\n\n\n\n\nmodel = sm.formula.logit(\"REMISS ~ CELL + SMEAR + INFIL + LI + BLAST + TEMP\", data=df).fit()\n\nprint(model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.399886\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 REMISS   No. Observations:                   27\nModel:                          Logit   Df Residuals:                       20\nMethod:                           MLE   Df Model:                            6\nDate:                Fri, 27 Sep 2024   Pseudo R-squ.:                  0.3718\nTime:                        16:24:16   Log-Likelihood:                -10.797\nconverged:                       True   LL-Null:                       -17.186\nCovariance Type:            nonrobust   LLR p-value:                   0.04670\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     64.2581     74.965      0.857      0.391     -82.670     211.187\nCELL          30.8301     52.135      0.591      0.554     -71.353     133.013\nSMEAR         24.6863     61.526      0.401      0.688     -95.903     145.275\nINFIL        -24.9745     65.281     -0.383      0.702    -152.923     102.974\nLI             4.3605      2.658      1.641      0.101      -0.849       9.570\nBLAST         -0.0115      2.266     -0.005      0.996      -4.453       4.430\nTEMP        -100.1734     77.753     -1.288      0.198    -252.567      52.220\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.11 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified."
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-2.",
    "href": "posts/hw7.1/index.html#문제-2.",
    "title": "HW7.1",
    "section": "문제 2.",
    "text": "문제 2.\n해당 모델은 통계적으로 유의한가요? 그 이유를 검정통계량를 사용해서 설명하시오.\n\nfrom scipy.stats import chi2\np_val=1-chi2.cdf(-2*(-17.186 + 10.797), df=6)\np_val\n\n0.04669995098322843\n\n\n\np-value: 0.04669995098322843 이므로 유의수준 0.05보다 작다 –&gt; 유의함"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-3.",
    "href": "posts/hw7.1/index.html#문제-3.",
    "title": "HW7.1",
    "section": "문제 3.",
    "text": "문제 3.\n유의수준이 0.2를 기준으로 통계적으로 유의한 변수는 몇개이며, 어느 변수 인가요?\n\nfrom scipy.stats import norm\n(1-norm.cdf(30.8301/52.135))*2 # cell 0.5542850639621408\n(1-norm.cdf(24.6863/61.526))*2 # smear 0.6882481267876992\n(norm.cdf(-24.9745/65.281))*2 # infil 0.702039210194285\n(1-norm.cdf(4.3605/2.658))*2 # li 0.10089726232517515 &lt; 0.2\n(norm.cdf(-0.0115/2.266))*2 # blast 0.9959507356304004 \n(norm.cdf(-100.1734/77.753))*2 # temp 0.19762271242612317 &lt; 0.2\n\n0.19762271242612317\n\n\n\n유의수준 0.2를 기준으로 통계적으로 유의한 변수는 2개, 유의한 변수는 LI(0.101), TEMP(0.198)이다."
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-4.",
    "href": "posts/hw7.1/index.html#문제-4.",
    "title": "HW7.1",
    "section": "문제 4.",
    "text": "문제 4.\n다음 환자에 대한 오즈는 얼마인가요?\n\n# CELL : 65%\n# SMEAR : 45%\n# INFIL : 55%\n# LI : 1.2\n# BLAST : 1.1세포/μL\n# TEMP : 0.9\nlog_odds = 64.2581 + 30.8301 * 0.65 + 24.6863 * 0.45 - 24.9745 * 0.55 + 4.3605 * 1.2 - 0.0115 * 1.1 - 100.1734 * 0.9\nodds = np.exp(log_odds)\nodds # 0.03817459641135519\n\n0.03817459641135519"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-5.",
    "href": "posts/hw7.1/index.html#문제-5.",
    "title": "HW7.1",
    "section": "문제 5.",
    "text": "문제 5.\n위 환자의 혈액에서 백혈병 세포가 관측되지 않은 확률은 얼마인가요?\n\np_hat = odds / (odds + 1)\np_hat # 0.03677088280074742)\n\n0.03677088280074742"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-6.",
    "href": "posts/hw7.1/index.html#문제-6.",
    "title": "HW7.1",
    "section": "문제 6.",
    "text": "문제 6.\nTEMP 변수의 계수는 얼마이며, 해당 계수를 사용해서 TEMP 변수가 백혈병 치료에 대한 영향을 설명하시오.\n\nnp.exp(-100.1734)*100 \n\n3.1278444454718354e-42\n\n\n\n계수: -100.1734 temp가 1 증가할때 REMISS에 대한 오즈가 3.1278444454718354e-42[%] 로 감소한다. 즉 백혈병 세포가 관측될 확률이 증가한다."
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-7.",
    "href": "posts/hw7.1/index.html#문제-7.",
    "title": "HW7.1",
    "section": "문제 7.",
    "text": "문제 7.\nCELL 변수의 99% 오즈비에 대한 신뢰구간을 구하시오.\n\nz = norm.ppf(0.995,0,1)\nr_ci = np.exp(30.8301 + z * 52.135)\nl_ci = np.exp(30.8301 - z * 52.135)\nl_ci, r_ci\n\n(1.1683218982002717e-45, 5.141881884993857e+71)"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-8.",
    "href": "posts/hw7.1/index.html#문제-8.",
    "title": "HW7.1",
    "section": "문제 8.",
    "text": "문제 8.\n주어진 데이터에 대하여 로지스틱 회귀 모델의 예측 확률을 구한 후, 50% 이상인 경우 1로 처리하여, 혼동 행렬를 구하시오.\n\ndf_x=df.drop([\"REMISS\"], axis=1)\ndf_y=df[\"REMISS\"]\n\ny_pred = model.predict(df_x).round().astype(int)\n\nfrom sklearn.metrics import confusion_matrix\nconf_mat=confusion_matrix(y_true=df_y, \n                          y_pred=y_pred,\n                          labels=[1, 0])\nconf_mat\n\narray([[ 5,  4],\n       [ 3, 15]], dtype=int64)"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-9.",
    "href": "posts/hw7.1/index.html#문제-9.",
    "title": "HW7.1",
    "section": "문제 9.",
    "text": "문제 9.\n해당 모델의 Accuracy는 얼마인가요?\n\n(conf_mat[0][0]+conf_mat[1][1])/conf_mat.sum() # 0.7407407407407407\n\n0.7407407407407407"
  },
  {
    "objectID": "posts/hw7.1/index.html#문제-10.",
    "href": "posts/hw7.1/index.html#문제-10.",
    "title": "HW7.1",
    "section": "문제 10.",
    "text": "문제 10.\n해당 모델의 F1 Score를 구하세요.\n\nprecision=conf_mat[0][0]/(conf_mat[0][0]+conf_mat[1][0])\nrecall=conf_mat[0][0]/(conf_mat[0][0]+conf_mat[0][1])\n\nf1=2/((1/precision)+(1/recall))\nf1 # 0.5882352941176471\n\n0.5882352941176471\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw6/index.html",
    "href": "posts/hw6/index.html",
    "title": "HW6",
    "section": "",
    "text": "슬통 자동차는 매해 출시되는 신형 자동차의 에너지 소비효율 등급을 1등급으로 유지하고 있다. 22년 개발된 신형 모델이 한국 자동차 평가원에서 설정한 에너지 소비 효율등급 1등급을 받을 수 있을지 검정하려한다. 평가원에 따르면 1등급의 기준은 평균 복합 에너지 소비효율이 16.0 이상인 경우 부여한다고 한다.\n다음은 신형 자동차 15대의 복합 에너지소비효율 측정한 결과이다.\n5.078, 15.752, 15.549, 15.56, 16.098, 13.277, 15.462, 16.116, 15.214, 16.93, 14.118, 14.927, 15.382, 16.709, 16.804\n표본에 의하여 판단해볼때, 현대자동차의 신형 모델은 에너지 효율 1등급으로 판단할 수 있을지 판단해보시오. (유의수준 1%로 설정)\n\n검정을 위한 가설을 명확하게 서술하시오.\n\n\nH_0 : mu&gt;=16\nH_A : mu&lt;16\n\n\n검정통계량 계산하시오.\n\n\nfrom scipy.stats import norm\nimport numpy as np\n\neff = np.array([15.078, 15.752, 15.549, 15.56, 16.098, 13.277, 15.462, 16.116, 15.214, 16.93, 14.118, 14.927, 15.382, 16.709, 16.804])\n\nx_bar=eff.mean()\ns=np.std(eff, ddof=1)\nn=len(eff)\n\nt=(x_bar-16)/(s/np.sqrt(n))\nprint('t = ',t)\n\nt =  -1.8500447456376756\n\n\n\np‑value을 구하세요.\n\n\nfrom scipy.stats import t\np_val =t.cdf(-1.85, df=n-1)\nprint('p_val = ',p_val) # 유의수준 0.01보다 큼\n\np_val =  0.0427658180508888\n\n\n\n현대자동차의 신형 모델의 평균 복합 에너지 소비효율에 대하여 95% 신뢰구간을 구해보세요.\n\n\nz_0025 = t.ppf(0.975, df=n-1)\n\nl_ci = x_bar - z_0025 * s/np.sqrt(n)\nr_ci = x_bar + z_0025 * s/np.sqrt(n)\nprint('l_ci = ',l_ci)\n\nl_ci =  14.988864240339733\n\n\n\nprint('r_ci = ',r_ci)\n\nr_ci =  16.074602426326933\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw6/index.html#신형-자동차의-에너지-소비효율-등급",
    "href": "posts/hw6/index.html#신형-자동차의-에너지-소비효율-등급",
    "title": "HW6",
    "section": "",
    "text": "슬통 자동차는 매해 출시되는 신형 자동차의 에너지 소비효율 등급을 1등급으로 유지하고 있다. 22년 개발된 신형 모델이 한국 자동차 평가원에서 설정한 에너지 소비 효율등급 1등급을 받을 수 있을지 검정하려한다. 평가원에 따르면 1등급의 기준은 평균 복합 에너지 소비효율이 16.0 이상인 경우 부여한다고 한다.\n다음은 신형 자동차 15대의 복합 에너지소비효율 측정한 결과이다.\n5.078, 15.752, 15.549, 15.56, 16.098, 13.277, 15.462, 16.116, 15.214, 16.93, 14.118, 14.927, 15.382, 16.709, 16.804\n표본에 의하여 판단해볼때, 현대자동차의 신형 모델은 에너지 효율 1등급으로 판단할 수 있을지 판단해보시오. (유의수준 1%로 설정)\n\n검정을 위한 가설을 명확하게 서술하시오.\n\n\nH_0 : mu&gt;=16\nH_A : mu&lt;16\n\n\n검정통계량 계산하시오.\n\n\nfrom scipy.stats import norm\nimport numpy as np\n\neff = np.array([15.078, 15.752, 15.549, 15.56, 16.098, 13.277, 15.462, 16.116, 15.214, 16.93, 14.118, 14.927, 15.382, 16.709, 16.804])\n\nx_bar=eff.mean()\ns=np.std(eff, ddof=1)\nn=len(eff)\n\nt=(x_bar-16)/(s/np.sqrt(n))\nprint('t = ',t)\n\nt =  -1.8500447456376756\n\n\n\np‑value을 구하세요.\n\n\nfrom scipy.stats import t\np_val =t.cdf(-1.85, df=n-1)\nprint('p_val = ',p_val) # 유의수준 0.01보다 큼\n\np_val =  0.0427658180508888\n\n\n\n현대자동차의 신형 모델의 평균 복합 에너지 소비효율에 대하여 95% 신뢰구간을 구해보세요.\n\n\nz_0025 = t.ppf(0.975, df=n-1)\n\nl_ci = x_bar - z_0025 * s/np.sqrt(n)\nr_ci = x_bar + z_0025 * s/np.sqrt(n)\nprint('l_ci = ',l_ci)\n\nl_ci =  14.988864240339733\n\n\n\nprint('r_ci = ',r_ci)\n\nr_ci =  16.074602426326933\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "HW4",
    "section": "",
    "text": "표본 분산 n-1 vs. n\n표본 분산 계산 시 왜 n-1로 나누는지 알아보도록 하겠습니다. 균일분포 (3, 7)에서 20개의 표본을 뽑아서 분산을 2가지 방법으로 추정해보세요.\n\nn-1로 나눈 것을 s_2, n으로 나눈 것을 k_2로 정의하고, s_2의 분포와 k_2의 분포를 그려주세요! (10000개 사용)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform\nimport seaborn as sns\nx=uniform.rvs(loc=3, scale=4, size=200000).reshape(10000,20)\nx.shape\ns_2=(((x - 5)**2) / (20-1)).sum(axis=1)\nsns.histplot(s_2)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nk_2=(((x - 5)**2) / 20).sum(axis=1)\nsns.histplot(k_2)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n각 분포 그래프에 모분산의 위치에 녹색 막대를 그려주세요.\n\n\nvar_x=np.var(x)\nsns.histplot(s_2)\nplt.axvline(x=var_x, color='red', linestyle='--', linewidth=1)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\nsns.histplot(k_2)\nplt.axvline(x=var_x, color='red', linestyle='--', linewidth=1)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n결과를 살펴보고, 왜 n-1로 나눈 것을 분산을 추정하는 지표로 사용하는 것이 타당한지 써주세요!\n\n\nn-1로 나눈 결과 분포의 평균이 모분산에 더 근접함\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw2.2/index.html",
    "href": "posts/hw2.2/index.html",
    "title": "HW2.2",
    "section": "",
    "text": "mpg데이터의 cty(도시 연비)와 hwy(고속도로 연비) 간에 어떤 관계가 있는지 알아보려고 합니다. x축은 cty, y축은 hwy로 된 산점도를 만들어 보세요.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmpg=pd.read_csv('../../data/mpg.csv')\nsns.scatterplot(data=mpg,\n                x='cty', y='hwy')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n미국의 지역별 인구통계 정보를 담은 midwest.csv를 이용해 전체 인구와 아시아인 인구 간에 어떤 관계가 있는지 알아보려고 합니다. x축은 poptotal(전체 인구), y축은 popasian(아시아인 인구)으로 된 산점도를 만들어 보세요. 전체 인구는 50만 명 이하, 아시아인 인구는 1만 명 이하인 지역만 산점도에 표시되게 설정하세요.\n\nmdw=pd.read_csv('../../data/midwest.csv')\nsns.scatterplot(data=mdw,\n                x='poptotal', y='popasian').set(xlim=[0,500000], ylim=[0,10000])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/hw2.2/index.html#mpg데이터와-midwest-데이터를-이용해-분석-문제를-해결해-보세요.",
    "href": "posts/hw2.2/index.html#mpg데이터와-midwest-데이터를-이용해-분석-문제를-해결해-보세요.",
    "title": "HW2.2",
    "section": "",
    "text": "mpg데이터의 cty(도시 연비)와 hwy(고속도로 연비) 간에 어떤 관계가 있는지 알아보려고 합니다. x축은 cty, y축은 hwy로 된 산점도를 만들어 보세요.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmpg=pd.read_csv('../../data/mpg.csv')\nsns.scatterplot(data=mpg,\n                x='cty', y='hwy')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n미국의 지역별 인구통계 정보를 담은 midwest.csv를 이용해 전체 인구와 아시아인 인구 간에 어떤 관계가 있는지 알아보려고 합니다. x축은 poptotal(전체 인구), y축은 popasian(아시아인 인구)으로 된 산점도를 만들어 보세요. 전체 인구는 50만 명 이하, 아시아인 인구는 1만 명 이하인 지역만 산점도에 표시되게 설정하세요.\n\nmdw=pd.read_csv('../../data/midwest.csv')\nsns.scatterplot(data=mdw,\n                x='poptotal', y='popasian').set(xlim=[0,500000], ylim=[0,10000])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/hw2.2/index.html#mpg데이터를-이용해-분석-문제를-해결해-보세요.",
    "href": "posts/hw2.2/index.html#mpg데이터를-이용해-분석-문제를-해결해-보세요.",
    "title": "HW2.2",
    "section": "mpg데이터를 이용해 분석 문제를 해결해 보세요.",
    "text": "mpg데이터를 이용해 분석 문제를 해결해 보세요.\n어떤 회사에서 생산한 ’suv’차종의 도시 연비가 높은지 알아보려고 합니다. ’suv’차종을 대상으로 cty(도시 연비) 평균이 가장 높은 회사 다섯 곳을 막대 그래프로 표현해 보세요. 막대는 연비가 높은 순으로 정렬하세요.\n\ndf_mpg=mpg.groupby('manufacturer', as_index=False)\\\n    .agg(cty_mean=('cty','mean')).\\\n    sort_values('cty_mean',ascending=False).head()\nsns.barplot(data=df_mpg, x='manufacturer', y='cty_mean', hue='manufacturer')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n자동차 중에 어떤 category(자동차 종류)가 많은지 알아보려고 합니다. sns.barplot()을 이용해 자동차 종류별 빈도를 표현한 막대 그래프를 만들어 보세요. 막대는 빈도가 높은 순으로 정렬하세요\n\ndf_mpg2=mpg.groupby('category', as_index=False)\\\n    .agg(cat_count=('category','count')).\\\n    sort_values('cat_count',ascending=False)\nsns.barplot(data=df_mpg2, x='category', y='cat_count', hue='category')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "HW2",
    "section": "",
    "text": "자동차 배기량에 따라 고속도로 연비가 다른지 알아보려고 합니다. displ(배기량)이 4이하인 자동차와 5이상인 자동차 중 어떤 자동차의 hwy(고속도로 연비) 평균이 더 높은지 알아보세요.\n\nimport pandas as pd\nimport numpy as np\nmpg=pd.read_csv('../../data/mpg.csv')\nmpg_l=mpg.query('displ&lt;=4')\nmpg_h=mpg.query('displ&gt;=5')\nmpg_l['hwy'].mean()\nmpg_h['hwy'].mean()\n\n18.07894736842105\n\n\n자동차 제조 회사에 따라 도시 연비가 어떻게 다른지 알아보려고 합니다. ’audi’와 ’toyota’중 어느 manufacturer(자동차 제조 회사)의 cty(도시 연비)평균이 더 높은지 알아보세요.\n\nmpg_au=mpg.query('manufacturer==\"audi\"')\nmpg_to=mpg.query('manufacturer==\"toyota\"')\nmpg_au['cty'].mean()\nmpg_to['cty'].mean()\n\n18.529411764705884\n\n\n‘chevrolet’,‘ford’,’honda’자동차의 고속도로 연비 평균을 알아보려고 합니다. 세 회사의 데이터를 추출한 다음 hwy 전체 평균을 구해 보세요.\n\nmpg_3=mpg.query('manufacturer in[\"chevrolet\",\"ford\",\"honda\"]')\nmpg_3['hwy'].mean()\n\n22.50943396226415"
  },
  {
    "objectID": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.144",
    "href": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.144",
    "title": "HW2",
    "section": "",
    "text": "자동차 배기량에 따라 고속도로 연비가 다른지 알아보려고 합니다. displ(배기량)이 4이하인 자동차와 5이상인 자동차 중 어떤 자동차의 hwy(고속도로 연비) 평균이 더 높은지 알아보세요.\n\nimport pandas as pd\nimport numpy as np\nmpg=pd.read_csv('../../data/mpg.csv')\nmpg_l=mpg.query('displ&lt;=4')\nmpg_h=mpg.query('displ&gt;=5')\nmpg_l['hwy'].mean()\nmpg_h['hwy'].mean()\n\n18.07894736842105\n\n\n자동차 제조 회사에 따라 도시 연비가 어떻게 다른지 알아보려고 합니다. ’audi’와 ’toyota’중 어느 manufacturer(자동차 제조 회사)의 cty(도시 연비)평균이 더 높은지 알아보세요.\n\nmpg_au=mpg.query('manufacturer==\"audi\"')\nmpg_to=mpg.query('manufacturer==\"toyota\"')\nmpg_au['cty'].mean()\nmpg_to['cty'].mean()\n\n18.529411764705884\n\n\n‘chevrolet’,‘ford’,’honda’자동차의 고속도로 연비 평균을 알아보려고 합니다. 세 회사의 데이터를 추출한 다음 hwy 전체 평균을 구해 보세요.\n\nmpg_3=mpg.query('manufacturer in[\"chevrolet\",\"ford\",\"honda\"]')\nmpg_3['hwy'].mean()\n\n22.50943396226415"
  },
  {
    "objectID": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.153",
    "href": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.153",
    "title": "HW2",
    "section": "mpg 데이터를 이용해 분석 문제를 해결해 보세요. (p.153)",
    "text": "mpg 데이터를 이용해 분석 문제를 해결해 보세요. (p.153)\n’audi’에서 생산한 자동차 중에 어떤 자동차 모델의 hwy(고속도로 연비)가 높은지 알아보려고 합니다. ’audi’에서 생산한 자동차 중 hwy가 1~5위에 해당하는 자동차의 데이터를 출력하세요.\n\nmpg.query('manufacturer==\"audi\"').sort_values('hwy',ascending=False).head()\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\ncategory\n\n\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n9\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact"
  },
  {
    "objectID": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.158",
    "href": "posts/hw2/index.html#mpg-데이터를-이용해-분석-문제를-해결해-보세요.-p.158",
    "title": "HW2",
    "section": "mpg 데이터를 이용해 분석 문제를 해결해 보세요. (p.158)",
    "text": "mpg 데이터를 이용해 분석 문제를 해결해 보세요. (p.158)\nmpg데이터 복사본을 만들고, cty와 hwy를 더한 ’합산 연비 변수’를 추가하세요.\n\nmpg_new=mpg.copy()\nmpg_new=mpg_new.assign(total=mpg['cty']+mpg['hwy'])\nmpg_new\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\ncategory\ntotal\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n47\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n50\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n51\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n51\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n47\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n50\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n42\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n44\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n43\n\n\n\n\n234 rows × 12 columns\n\n\n\n앞에서 만든 ’합산 연비 변수’를 2로 나눠 ’평균 연비 변수’를 추가하세요.\n\nmpg_new=mpg_new.assign(mean=mpg_new['total']/2)\nmpg_new\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\ncategory\ntotal\nmean\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n47\n23.5\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n50\n25.0\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n51\n25.5\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n51\n25.5\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n42\n21.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n47\n23.5\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n50\n25.0\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n42\n21.0\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n44\n22.0\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n43\n21.5\n\n\n\n\n234 rows × 13 columns\n\n\n\n’평균 연비 변수’가 가장 높은 자동차 3종의 데이터를 출력하세요.\n\nmpg_new.sort_values('mean',ascending=False).head(3)\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\ncategory\ntotal\nmean\n\n\n\n\n221\nvolkswagen\nnew beetle\n1.9\n1999\n4\nmanual(m5)\nf\n35\n44\nd\nsubcompact\n79\n39.5\n\n\n212\nvolkswagen\njetta\n1.9\n1999\n4\nmanual(m5)\nf\n33\n44\nd\ncompact\n77\n38.5\n\n\n222\nvolkswagen\nnew beetle\n1.9\n1999\n4\nauto(l4)\nf\n29\n41\nd\nsubcompact\n70\n35.0\n\n\n\n\n\n\n\n1~3번 문제를 해결할 수 있는 하나로 연결된 pandas구문을 만들어 실행해보세요. 데이터는 복사본 대신 mpg파일을 이용하세요.\n\nmpg.assign(total= lambda x: x['hwy']+x['cty'],\n    mean= lambda x: x['total']/2)\\\n    .sort_values('mean',ascending=False)\\\n    .head(3)\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\ncategory\ntotal\nmean\n\n\n\n\n221\nvolkswagen\nnew beetle\n1.9\n1999\n4\nmanual(m5)\nf\n35\n44\nd\nsubcompact\n79\n39.5\n\n\n212\nvolkswagen\njetta\n1.9\n1999\n4\nmanual(m5)\nf\n33\n44\nd\ncompact\n77\n38.5\n\n\n222\nvolkswagen\nnew beetle\n1.9\n1999\n4\nauto(l4)\nf\n29\n41\nd\nsubcompact\n70\n35.0\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/houseprice_nn/index.html",
    "href": "posts/houseprice_nn/index.html",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "",
    "text": "House Prices - Advanced Regression Techniques"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#kaggle-링크",
    "href": "posts/houseprice_nn/index.html#kaggle-링크",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "",
    "text": "House Prices - Advanced Regression Techniques"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#전처리",
    "href": "posts/houseprice_nn/index.html#전처리",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "전처리",
    "text": "전처리\n\n# 필요한 패키지 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\nnp.random.seed(20240911) \n\n## 필요한 데이터 불러오기\nhouse_train=pd.read_csv(\"./train.csv\")\nhouse_test=pd.read_csv(\"./test.csv\")\nsub_df=pd.read_csv(\"./sample_submission.csv\")\n\n## NaN 채우기\n# 각 숫치형 변수는 평균 채우기\n# 각 범주형 변수는 Unknown 채우기\nhouse_train.isna().sum()\nhouse_test.isna().sum()\n\n## 숫자형 채우기\nquantitative = house_train.select_dtypes(include = [int, float])\nquantitative.isna().sum()\nquant_selected = quantitative.columns[quantitative.isna().sum() &gt; 0]\n\nfor col in quant_selected:\n    house_train[col] = house_train[col].fillna(house_train[col].mean())\nhouse_train[quant_selected].isna().sum()\n\n## 범주형 채우기\nqualitative = house_train.select_dtypes(include = [object])\nqualitative.isna().sum()\nqual_selected = qualitative.columns[qualitative.isna().sum() &gt; 0]\n\nfor col in qual_selected:\n    house_train[col] = house_train[col].fillna(\"unknown\")\n\n\n# test 데이터 채우기\n## 숫자형 채우기\nquantitative = house_test.select_dtypes(include = [int, float])\nquantitative.isna().sum()\nquant_selected = quantitative.columns[quantitative.isna().sum() &gt; 0]\n\nfor col in quant_selected:\n    house_test[col] = house_test[col].fillna(house_train[col].mean())\nhouse_test[quant_selected].isna().sum()\n\n## 범주형 채우기\nqualitative = house_test.select_dtypes(include = [object])\nqualitative.isna().sum()\nqual_selected = qualitative.columns[qualitative.isna().sum() &gt; 0]\n\nfor col in qual_selected:\n    house_test[col] = house_test[col].fillna(\"unknown\")\nhouse_test[qual_selected].isna().sum()\n\n\nhouse_train.shape\nhouse_test.shape\ntrain_n=len(house_train)\n\n# 통합 df 만들기 + 더미코딩\n# house_test.select_dtypes(include=[int, float])\n\ndf = pd.concat([house_train, house_test], ignore_index=True)\n# df.info()\ndf = pd.get_dummies(\n    df,\n    columns= df.select_dtypes(include=[object]).columns,\n    drop_first=True\n    )\ndf\n\n# train / test 데이터셋\ntrain_df=df.iloc[:train_n,]\ntest_df=df.iloc[train_n:,]\n\n## 이상치 탐색\ntrain_df=train_df.query(\"GrLivArea &lt;= 4500\")\n\n## train\ntrain_x=train_df.drop(\"SalePrice\", axis=1)\ntrain_y=train_df[\"SalePrice\"]\n\n## test\ntest_x=test_df.drop(\"SalePrice\", axis=1)"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#모델-생성",
    "href": "posts/houseprice_nn/index.html#모델-생성",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "모델 생성",
    "text": "모델 생성\n\n# 신경망 모델 정의\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=train_x.shape[1], activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1))\n\n# 모델 컴파일\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\n# 모델 학습\nhistory = model.fit(train_x, train_y, epochs=200, batch_size=32, validation_split=0.1, verbose=1)"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#학습-과정-시각화",
    "href": "posts/houseprice_nn/index.html#학습-과정-시각화",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "학습 과정 시각화",
    "text": "학습 과정 시각화\n\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#예측",
    "href": "posts/houseprice_nn/index.html#예측",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "예측",
    "text": "예측\n\n# 예측\npred_y = model.predict(test_x).flatten()\n\n\n# SalePrice 바꿔치기\nsub_df[\"SalePrice\"] = pred_y\nsub_df.head()\n\n\n\n\n\n\n\n\nId\nSalePrice\n\n\n\n\n0\n1461\n146493.953125\n\n\n1\n1462\n178756.421875\n\n\n2\n1463\n187970.453125\n\n\n3\n1464\n190732.937500\n\n\n4\n1465\n168399.859375"
  },
  {
    "objectID": "posts/houseprice_nn/index.html#모델-성능",
    "href": "posts/houseprice_nn/index.html#모델-성능",
    "title": "Kaggle: Houseprice - Tensorflow",
    "section": "모델 성능",
    "text": "모델 성능\n\n# 모델 성능 평가\nfrom sklearn.metrics import mean_squared_error\ntrain_y_pred = model.predict(train_x)\nmse = mean_squared_error(train_y, train_y_pred)\n\n\nprint(f\"Mean Squared Error on training set: {mse:.2f}\")\n\nMean Squared Error on training set: 852977100.51\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html",
    "href": "posts/Ames_project/project_rev4.html",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "",
    "text": "import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport json\nimport folium\nimport re\nfrom fuzzywuzzy import process\nfrom branca.element import IFrame\n\n# 데이터 로드\ndf_iowa = pd.read_excel('./data/Iowa ACT 5 Year Trends by District for Graduating Classes 2019 to 2023 (2).xlsx')\ngeo_iowa = json.load(open('./data/Iowa_School_Districts_2023-2024.geojson', encoding='UTF-8'))\ndf =pd.read_csv('./data/houseprice-with-lonlat.csv')\nhouse_train=pd.read_csv(\"./data/train.csv\")\nmajor = pd.read_csv('./data/Majors Awarded.csv')\ndf_sex=pd.read_csv('./data/Sex_Breakdown_for_Common_Majors.csv')\ndf_major = pd.read_csv('./data/Common_Jobs_by_Major.csv')\n\n\nbins=np.array([1870,1879,1889,1899,1909,1919,1929,1939,\n               1949,1959,1969,1979,1989,1999,2009,2019])\ndf['Years']=pd.cut(df['Year_Built'], bins,\n                       labels=(np.arange(187,202)*10).astype(int))\n\nyears_roof = df.groupby(['Years','Roof_Style'])\\\n               .agg(count_roof=('Roof_Style', 'count'))\\\n               .reset_index()\n\nfig1 = px.bar(\n    years_roof,\n    x='Years',\n    y='count_roof',\n    color='Roof_Style',\n    title='Roof Styles over Years',\n    labels={'Roof_Style': 'Roof Style', 'count_roof': 'Count'},\n    barmode='stack'\n)\n\nfig1.update_layout(\n    showlegend=True,  # 전체 레이아웃에서 범례 표시\n    margin=dict(t=100, b=50, l=50, r=150)\n)\n\n\nyears_ext = df.groupby(['Years','Exterior_1st'])\\\n               .agg(count_ext=('Exterior_1st', 'count'))\\\n               .reset_index()\n\nfig2 = px.bar(\n    years_ext,\n    x='Exterior_1st',\n    y='count_ext',\n    animation_frame='Years',\n    title='Exterior over Years',\n    color=\"Exterior_1st\",\n    labels={'Exterior_1st': 'Exterior', 'count_ext': 'Count'}\n)\n\nfig2.update_layout(\n    width=1100,  # 그래프 너비\n    height=600,  # 그래프 높이\n    showlegend=True,  # 전체 레이아웃에서 범례 표시\n    margin=dict(t=100, b=50, l=50, r=150)\n)\n\n\n\n\n에임스(Ames)는 아이오와 주립대학교를 중심으로 교육과 연구가 활발히 이루어지는 도시로, 농업과 공학 분야에서의 혁신이 두드러진다.\n높은 삶의 질을 자랑하며, 매년 열리는 아이오와 주립대학교의 다양한 문화 행사와 지역 사회 활동으로도 유명하다.\n리버밸리 파크(River Valley Park)와 같은 넓은 자연 공간이 있으며, 전통적인 대학 건물과 현대적인 연구 시설이 조화를 이루는 외관을 자랑한다.\n\n\n\n\n\n\n\n꾸준하게 Gable 형태의 지붕스타일을 선호하고 있으며, 2000년도에 들어서면서 HIP 형태의 지붕스타일의 선호도가 크게 증가했음\n\n\n\n\n\nfig1.show()\n\n                                                \n\n\n\n\n\n\n\n\n\n\n과거부터 지속적으로 MetalSd 및 WdSdng가 외장재로 많이 사용되어 왔으며, 1950년대 이후로 HdBoard의 사용량이 크게 증가했다가 2000년대 이후로는 CemntBd의 사용량이 증가하였음\n\n\n\n\n\nfig2.show()"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row",
    "href": "posts/Ames_project/project_rev4.html#row",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "",
    "text": "에임스(Ames)는 아이오와 주립대학교를 중심으로 교육과 연구가 활발히 이루어지는 도시로, 농업과 공학 분야에서의 혁신이 두드러진다.\n높은 삶의 질을 자랑하며, 매년 열리는 아이오와 주립대학교의 다양한 문화 행사와 지역 사회 활동으로도 유명하다.\n리버밸리 파크(River Valley Park)와 같은 넓은 자연 공간이 있으며, 전통적인 대학 건물과 현대적인 연구 시설이 조화를 이루는 외관을 자랑한다."
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-1",
    "href": "posts/Ames_project/project_rev4.html#row-1",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "",
    "text": "꾸준하게 Gable 형태의 지붕스타일을 선호하고 있으며, 2000년도에 들어서면서 HIP 형태의 지붕스타일의 선호도가 크게 증가했음\n\n\n\n\n\nfig1.show()"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-2",
    "href": "posts/Ames_project/project_rev4.html#row-2",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "",
    "text": "과거부터 지속적으로 MetalSd 및 WdSdng가 외장재로 많이 사용되어 왔으며, 1950년대 이후로 HdBoard의 사용량이 크게 증가했다가 2000년대 이후로는 CemntBd의 사용량이 증가하였음\n\n\n\n\n\nfig2.show()"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-3",
    "href": "posts/Ames_project/project_rev4.html#row-3",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "Row",
    "text": "Row\nIowa State University 아이오와 주립 대학교\n아이오와 주, Ames시에 소재하고 있는 연구중심 공립 종합대학\n미국 내 최상위 130여개의 대학 중 한 곳으로 연구와 교육 분야에서 높은 평가를 받고 있다.\n대표적으로 이공계가 강한 학교 중 하나로 손꼽히며 공학 분야가 전반적으로 우수하고, 그 외에도 생명공학, 통계학, 컴퓨터 공학, 원자력 등 다양한 분야의 학문적 명성도 뛰어나다.\n아이오와 주립대학교는 지역 커뮤니티 프로그램, 지역 사회와의 협력을 통해 에임스시에 긍정적인 영향을 주고 있다.\n\n# 데이터 전처리\ndf_iowa.columns = df_iowa.iloc[1].rename(None)\ndf_iowa = df_iowa.iloc[2:, :]\ndf_iowa.replace('Small N', np.nan, inplace=True)\ndf_iowa.dropna(inplace=True)\ndf_iowa.reset_index(drop=True, inplace=True)\ndf_iowa['CRB % All Four'] = pd.to_numeric(df_iowa['CRB % All Four'])\n \n# 평균 점수 계산\ngrade_mean = df_iowa.groupby('District Name').agg(score_mean=('CRB % All Four', 'mean')).reset_index()\n \n# 정규화 함수 정의\ndef normalize_name(name):\n   name = name.upper()\n   name = re.sub(r'\\s+', ' ', name)  # 다중 공백을 단일 공백으로\n   name = re.sub(r'[^A-Z\\s]', '', name)  # 알파벳과 공백 제외 모든 문자 제거\n   return name.strip()\n\n# GeoJSON 구역 이름 정규화\ngeojson_areas = {normalize_name(feature['properties']['DistrictName']): feature['properties']['DistrictName'] for feature in geo_iowa['features']}\n \n# DataFrame 구역 이름 정규화\ndf_iowa['Normalized'] = df_iowa['District Name'].apply(normalize_name)\n\n# 유사도 비교 함수 정의\ndef find_best_match(name, choices):\n   return process.extractOne(name, choices)[0]\n\n# GeoJSON 구역 이름과 DataFrame 구역 이름 매칭\nname_mapping = {}\nfor name in df_iowa['Normalized']:\n   best_match = find_best_match(name, geojson_areas.keys())\n   name_mapping[name] = geojson_areas[best_match]\n \n# DataFrame의 통일된 구역 이름 적용\ndf_iowa['Unified District'] = df_iowa['Normalized'].map(name_mapping)\n\n# 통합된 데이터프레임과 GeoJSON 데이터 매칭\ngrade_mean = df_iowa.groupby('Unified District').agg(score_mean=('CRB % All Four', 'mean')).reset_index()\n\n# Folium 맵 생성\nmap_iowa = folium.Map(\n   location=[42.03, -93.64289689856655],\n   zoom_start=8, \n   tiles='cartodbpositron'\n)\n \n# Folium Choropleth 추가\nfolium.Choropleth(\n   geo_data=geo_iowa,\n   data=grade_mean,\n   columns=['Unified District', 'score_mean'],\n   key_on='feature.properties.DistrictName',  # GeoJSON의 실제 속성 이름\n   fill_color='PuBu',\n   fill_opacity=0.7,\n   line_opacity=0.2,\n   legend_name='Average Score'\n).add_to(map_iowa)\n \ndf_ames = df_iowa.query('`District Name`==\"AMES COMMUNITY SCHOOL DISTRICT\"')\ndf_ames = pd.melt(\n   df_ames,\n   id_vars='Grad Year',\n   value_vars=['Avg Eng', 'Avg Math', 'Avg Reading', 'Avg Sci', 'Avg Comp'],  # 긴 형식으로 변환할 열들\n   var_name='Subject',  # 과목명을 담을 열 이름\n   value_name='Average Score'  # 각 과목의 평균 성적을 담을 열 이름\n)\n \nfig_score = px.scatter(\n   df_ames,\n   x='Grad Year',\n   y='Average Score',\n   color='Subject',  # 과목별로 색상 다르게 설정\n   title='과목별 평균성적',\n   labels={'Average Score': '평균성적', 'Subject': '과목', 'Grad Year': '졸업년도'},\n   trendline='ols'  # 추세선 추가 (선택사항)\n)\n \n# Plotly 그래프를 HTML로 변환\nfig_html = fig_score.to_html(include_plotlyjs='cdn')\n# IFrame 생성 및 Folium Popup에 추가\niframe = IFrame(html=fig_html, width=700, height=500)\npopup = folium.Popup(iframe, max_width=700)\n \n# Marker 추가\nfolium.Marker(\n   location=[42.054035, -93.64289689856655],\n   popup=popup,\n   icon=folium.Icon(color='darkpurple')\n).add_to(map_iowa)\n\n\nmajor[\"Completions\"] = major[\"Completions\"].astype(int)\n\n\nfig3 = px.treemap(major,\n                 path=[px.Constant('All Majors'), 'CIP2', 'CIP4'], \n                 values='Completions',\n                 color='CIP2',  \n                 hover_data=['Completions'], \n                 color_discrete_sequence=   px.colors.qualitative.Pastel2 + px.colors.qualitative.Set3 + px.colors.qualitative.Pastel\n                )\n\n# 선택/해제 가능한 항목의 설정\nfig3.update_layout(\n    clickmode='event+select',  # 클릭 시 이벤트와 선택 모드 활성화\n    dragmode='select'   # 드래그 모드 설정\n)\n\n\ndf_sex = df_sex[['Year', 'Sex', 'CIP6', 'Completions']]\n\n# 전공명으로 CIP6을 변환하는 사전 정의\ndf_sex['CIP6'] = df_sex['CIP6'].replace({\n   '110103': '정보통신기술',\n    '141901': '기계공학',\n    '520203': '물류공급망관리',\n    '520801': '일반금융',\n    '521401': '일반마케팅_관리'\n})\n\n# 성별과 전공별로 Completions을 집계합니다.\ndf_grouped = df_sex.groupby(['Year', 'Sex', 'CIP6'], as_index=False).sum()\n\nfig4 = px.line(\n    df_grouped,\n    x='Year',\n    y='Completions',\n    color='CIP6',\n    facet_col='Sex',\n    line_group='CIP6',\n    markers=True,\n    title=\"연도별 성별에 따른 전공 선호도 선 그래프\",\n    labels={\"Year\": \"연도\", \"Completions\": \"전공 선호도 (완료 수)\"}\n  \n)\n\n# 레이아웃 업데이트\nfig4.update_layout(\n    width=900,  # 그래프 너비\n    height=450,  # 그래프 높이\n    legend_title=\"전공\",\n    showlegend=True,\n    margin=dict(t=100, b=20, l=20, r=20)\n)\n\n\n# 필요한 열만 선택\ndf_major = df_major[['Detailed Occupation', 'Total Population', 'ID CIP2']]\n\n# CIP2 코드 숫자를 한글 전공명으로 변경\ndf_major['ID CIP2'] = df_major['ID CIP2'].replace({\n    14: '공학',\n    19: '인문과학',\n    27: '수학_통계',\n    31: '공원_레크리에이션_레저',\n    26: '생물학'\n})\n\n# Detailed Occupation 변수를 한글 직업군으로 변경\ndf_major['Detailed Occupation'] = df_major['Detailed Occupation'].replace({\n    'Other managers': '기타경영자',\n    'Software developers': '소프트웨어개발자',\n    'Civil engineers': '토목기사',\n    'Miscellaneous engineers, including nuclear engineers': '기타기술자',\n    'Physicians': '의료인'\n})\n\n# 전공별로 필터링\ndf_engineering = df_major[df_major['ID CIP2'] == '공학']\ndf_human_sciences = df_major[df_major['ID CIP2'] == '인문과학']\ndf_math_statistics = df_major[df_major['ID CIP2'] == '수학_통계']\ndf_parks_recreation_leisure = df_major[df_major['ID CIP2'] == '공원_레크리에이션_레저']\ndf_biology = df_major[df_major['ID CIP2'] == '생물학']"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-4",
    "href": "posts/Ames_project/project_rev4.html#row-4",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "Row",
    "text": "Row\n\ndict(\n  value = \"1858\",\n  )\n\n{'value': '1858'}\n\n\n\ndict(\n  value = \"Tier 1\",\n  )\n\n{'value': 'Tier 1'}\n\n\n\ndict(\n  value = \"29,542\",\n  style = \"font-size: 10px;\"\n  )\n\n{'value': '29,542', 'style': 'font-size: 10px;'}\n\n\n\ndict(\n  value = \"8,469\"\n  )\n\n{'value': '8,469'}"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-5",
    "href": "posts/Ames_project/project_rev4.html#row-5",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "Row",
    "text": "Row\n\nCol\n\nmap_iowa\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAmes Community School District의 교육 수준이 우수함\n\n\nCol\n\n전공자 수 (2012 ~ 2022)성별과 연도에 따른 전공자 수 변화\n\n\n\nfig3.show()\n\n                                                \n\n\n\nEngineering분야가 많고, 전체 전공 중 Mechnical Engineering전공자수가 가장 많음\n\n\n\n\nfig4.show()\n\n                                                \n\n\n\n남자는 Mechnical Engineering, 여자는 Marketing 관련 전공이 가장 많음\n2018년 이후 Information Technology 전공자 수가 크게 늘어났는데, AI 기술 발전의 영향으로 해석됨"
  },
  {
    "objectID": "posts/Ames_project/project_rev4.html#row-6",
    "href": "posts/Ames_project/project_rev4.html#row-6",
    "title": "Ames 특징 및 교육 대쉬보드",
    "section": "Row",
    "text": "Row\n\nCol\n\n# 서브플롯 생성\nfig_subplot = make_subplots(\n    rows=1, cols=5,\n    subplot_titles=('Engineering', 'Human sciences', 'Math & Statistics', 'Parks, Recreation, & Leisure', 'Biology'),\n    specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"},{\"type\": \"pie\"}, {\"type\": \"pie\"}]]\n)\n\n# 각 전공별 파이 차트 추가\ndef add_pie_chart(df, title, row, col):\n    df_pie = df.groupby('Detailed Occupation').sum('Total Population').reset_index()\n    fig_subplot.add_trace(go.Pie(\n        labels=df_pie['Detailed Occupation'],\n        values=df_pie['Total Population'],\n        name=title,\n        textinfo='label+percent',  # 레이블과 퍼센트를 표시\n        insidetextorientation='auto'  # 텍스트가 겹치지 않도록 자동 조정\n    ), row=row, col=col)\n\nadd_pie_chart(df_engineering, 'Engineering', 1, 1)\nadd_pie_chart(df_human_sciences, 'Human sciences', 1, 2)\nadd_pie_chart(df_math_statistics, 'Math & Statistics', 1, 3)\nadd_pie_chart(df_parks_recreation_leisure, 'Parks, Recreation, & Leisure', 1, 4)\nadd_pie_chart(df_biology, 'Biology', 1, 5)\n\n# 레이아웃 업데이트\nfig_subplot.update_layout(\n    height=800,  # 그래프 높이 설정\n    showlegend=True,  # 전체 레이아웃에서 범례 표시\n    legend=dict(\n        orientation=\"h\",  # 수평으로 범례 표시\n        yanchor=\"bottom\",  # 범례의 수직 앵커\n        y=0.1,  # 범례의 수직 위치\n        xanchor=\"center\",  # 범례의 수평 앵커\n        x=0.5,  # 범례의 수평 위치\n        traceorder=\"reversed\"  # 범례 항목의 순서\n    )\n)\n\nfig_subplot.show()\n\n\nfig_subplot.show()\n\n                                                \n\n\n소프트웨어 및 의료 종사자가 많은것을 알 수 있음"
  },
  {
    "objectID": "KNU_blog.html",
    "href": "KNU_blog.html",
    "title": "KNU",
    "section": "",
    "text": "All In One PET 수거기\n\n\n\nproject\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#summary",
    "href": "about.html#summary",
    "title": "About",
    "section": "Summary",
    "text": "Summary\nPassionate Electrical Engineering student with a strong foundation in electrical and electronic engineering, specializing in power systems and electronic circuit design."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nKyungpook National Univercity | Daegu, South Korea Bachelor of Electrical Engineering | Mar 2023(transfer) - present GPA : 4.1/4.5\nKorea Polytechnics | Seongnam, South Korea Associate of Electrical Engineering | Mar 2019- Feb 2023 GPA : 4.0/4.5\nCollege of English Language | San Diego, USA English Language Course | Jan 2024 - Mar 2024"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nRepublic of Korean army | Discharged as a sergeant | Jun 2020 - Dec 2021"
  },
  {
    "objectID": "about.html#awards-honors",
    "href": "about.html#awards-honors",
    "title": "About",
    "section": "Awards & Honors",
    "text": "Awards & Honors\n\nGold Prize in Creative Engineering Design Competition, Kyungpook National University"
  },
  {
    "objectID": "about.html#certificates-other-skills",
    "href": "about.html#certificates-other-skills",
    "title": "About",
    "section": "Certificates & Other Skills",
    "text": "Certificates & Other Skills\n\nConversation English- OPIc IH certified in May 2024\nProven English proficiency for business with a total score of 965 in TOEIC\nIndustrial Engineer Electricity (national technical qualification)\nCompleted ‘High-speed System Design and Understanding Signal/Power Integrity’ training by IC Design Education Center\nCompleted ‘Basic Training Program for Foundry-Oriented Semiconductor Design’ training by Korea Semiconducter Academy"
  },
  {
    "objectID": "KNU/PET_project/PET_project.html#문제정의-및-trend-분석",
    "href": "KNU/PET_project/PET_project.html#문제정의-및-trend-분석",
    "title": "All In One PET 수거기",
    "section": "문제정의 및 Trend 분석",
    "text": "문제정의 및 Trend 분석\n\n많은 카페와 커피용기를 들고 다니는 사람들이 관찰됨, 길거리에는 무분별하게 버려진 커피용기와 PET용기가 많이 보임.\n\n재활용을 통해 탄소배출을 줄이는 것이 중요한데, 커피용기와 PET용기를 세척하지 않고 버리면 재활용이 불가능함.\n\nPET용기를 세척 후 수거하고 압축 시스템을 추가해 더 많은 양을 수용할 수 있는 수거기를 고안, 태양광 에너지를 이용해 친환경적으로 제품을 작동시킴."
  },
  {
    "objectID": "KNU/PET_project/PET_project.html#작품도",
    "href": "KNU/PET_project/PET_project.html#작품도",
    "title": "All In One PET 수거기",
    "section": "작품도",
    "text": "작품도\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "lsbigdata_blog.html",
    "href": "lsbigdata_blog.html",
    "title": "LS bigdata",
    "section": "",
    "text": "Recent posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle: Houseprice -stacking\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle: Houseprice - Tensorflow\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nYOLO8\n\n\n\nDeeplearning\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW7.1\n\n\n\nbigdata\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW7\n\n\n\nbigdata\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAmes 특징 및 교육 대쉬보드\n\n\n\nproject\n\n\n\n\n\n\n\n무적 1조\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW6\n\n\n\nbigdata\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nAug 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n계절별 화재발생 빈도 및 요인 분석\n\n\n\nproject\n\n\n\n\n\n\n\n명예소방관 시켜조🚒 : 안상후, 오서연, 김주영, 김재희\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW5\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW4\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW3\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2.2\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2.1\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW1\n\n\n\nHomework\n\n\n\n\n\n\n\nSanghoo Ahn\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fire_project/fire_presentation.html#주제-선정-이유",
    "href": "posts/fire_project/fire_presentation.html#주제-선정-이유",
    "title": "계절별 화재발생 빈도 및 요인 분석",
    "section": "0. 주제 선정 이유",
    "text": "0. 주제 선정 이유\n\n화재에 의한 피해 감소를 위해 화재가 많이 발생하는 계절, 화재 발생에 많이 기여하는 요인, 인명피해가 많이 발생하는 요인을 분석하여 예방책 제안"
  },
  {
    "objectID": "posts/fire_project/fire_presentation.html#계절별-화재발생-빈도",
    "href": "posts/fire_project/fire_presentation.html#계절별-화재발생-빈도",
    "title": "계절별 화재발생 빈도 및 요인 분석",
    "section": "1. 계절별 화재발생 빈도",
    "text": "1. 계절별 화재발생 빈도\n\n&lt;데이터 설명&gt;\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"fire.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n항목\n2020\n2020.1\n2020.2\n2020.3\n2020.4\n2020.5\n2020.6\n2020.7\n2020.8\n...\n2022.3\n2022.4\n2022.5\n2022.6\n2022.7\n2022.8\n2022.9\n2022.10\n2022.11\n2022.12\n\n\n\n\n0\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n...\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n제품결함\n미상\n\n\n1\n합계\n38659\n9329\n4053\n630\n141\n458\n19186\n600\n238\n...\n686\n151\n430\n19666\n516\n214\n400\n346\n167\n3670\n\n\n2\n1월\n3100\n781\n377\n43\n17\n34\n1453\n44\n2\n...\n52\n20\n38\n2173\n48\n4\n37\n31\n9\n381\n\n\n3\n2월\n3011\n748\n342\n46\n15\n40\n1465\n35\n2\n...\n43\n12\n30\n2396\n52\n11\n34\n37\n5\n395\n\n\n4\n3월\n4053\n702\n343\n56\n12\n34\n2443\n55\n13\n...\n40\n9\n26\n1694\n39\n3\n46\n32\n9\n301\n\n\n\n\n5 rows × 38 columns\n\n\n\n\n\n\n&lt;데이터 전처리&gt;\n\n연도별로 분리\n\n\ndata_2020 = df[['항목'] + df.filter(like='2020').columns.tolist()]\n\ndata_2021 = df[['항목'] + df.filter(like='2021').columns.tolist()]\n\ndata_2022 = df[['항목'] + df.filter(like='2022').columns.tolist()]\n\ndata_2022.head()\n\n\n\n\n\n\n\n\n항목\n2022\n2022.1\n2022.2\n2022.3\n2022.4\n2022.5\n2022.6\n2022.7\n2022.8\n2022.9\n2022.10\n2022.11\n2022.12\n\n\n\n\n0\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n제품결함\n미상\n\n\n1\n합계\n40113\n10011\n3856\n686\n151\n430\n19666\n516\n214\n400\n346\n167\n3670\n\n\n2\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n9\n381\n\n\n3\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n5\n395\n\n\n4\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n9\n301\n\n\n\n\n\n\n\n\n\n2022년의 제품결함 열 삭제\n\n\ndata_2022 = data_2022.drop(columns = \"2022.11\")\ndata_2022.head()\n\n\n\n\n\n\n\n\n항목\n2022\n2022.1\n2022.2\n2022.3\n2022.4\n2022.5\n2022.6\n2022.7\n2022.8\n2022.9\n2022.10\n2022.12\n\n\n\n\n0\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n\n\n1\n합계\n40113\n10011\n3856\n686\n151\n430\n19666\n516\n214\n400\n346\n3670\n\n\n2\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n381\n\n\n3\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n395\n\n\n4\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n301\n\n\n\n\n\n\n\n\n\n0번째 행을 column으로 지정\n\n\ndata_2020.columns = data_2020.iloc[0] \n\ndata_2021.columns = data_2021.iloc[0] \n\ndata_2022.columns = data_2022.iloc[0] \n\ndata_2022.head()\n\n\n\n\n\n\n\n\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n\n\n\n\n0\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n\n\n1\n합계\n40113\n10011\n3856\n686\n151\n430\n19666\n516\n214\n400\n346\n3670\n\n\n2\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n381\n\n\n3\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n395\n\n\n4\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n301\n\n\n\n\n\n\n\n\n\n0행, 1행 삭제 후 인덱스 초기화\n\n\ndata_2020 = data_2020[2:]\ndata_2020 = data_2020.reset_index(drop=True)\n\ndata_2021 = data_2021[2:]\ndata_2021 = data_2021.reset_index(drop=True)\n\ndata_2022 = data_2022[2:]\ndata_2022 = data_2022.reset_index(drop=True)\n\ndata_2022.head()\n\n\n\n\n\n\n\n\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n\n\n\n\n0\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n381\n\n\n1\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n395\n\n\n2\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n301\n\n\n3\n4월\n3729\n713\n319\n55\n15\n37\n2112\n45\n12\n32\n40\n335\n\n\n4\n5월\n4105\n687\n287\n82\n14\n33\n2479\n41\n14\n39\n34\n382\n\n\n\n\n\n\n\n\n\n데이터타입이 object로 되어 있음\n\n\ndata_2022.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   항목      12 non-null     object\n 1   계       12 non-null     object\n 2   전기적요인   12 non-null     object\n 3   기계적요인   12 non-null     object\n 4   화학적요인   12 non-null     object\n 5   가스누출    12 non-null     object\n 6   교통사고    12 non-null     object\n 7   부주의     12 non-null     object\n 8   기타      12 non-null     object\n 9   자연적요인   12 non-null     object\n 10  방화      12 non-null     object\n 11  방화의심    12 non-null     object\n 12  미상      12 non-null     object\ndtypes: object(13)\nmemory usage: 1.3+ KB\n\n\n\n\nfor문을 사용해 데이터타입을 int로 변경\n\n\ncolumns_to_convert = ['계', '전기적요인', '기계적요인',\n                '화학적요인', '가스누출', '교통사고', '부주의', '기타',\n                '자연적요인', '방화', '방화의심', '미상']\n\nfor column in columns_to_convert:\n    data_2020[column] = pd.to_numeric(data_2020[column])\n\nfor column in columns_to_convert:\n    data_2021[column] = pd.to_numeric(data_2021[column])\n\nfor column in columns_to_convert:\n    data_2022[column] = pd.to_numeric(data_2022[column])\n\ndata_2022.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   항목      12 non-null     object\n 1   계       12 non-null     int64 \n 2   전기적요인   12 non-null     int64 \n 3   기계적요인   12 non-null     int64 \n 4   화학적요인   12 non-null     int64 \n 5   가스누출    12 non-null     int64 \n 6   교통사고    12 non-null     int64 \n 7   부주의     12 non-null     int64 \n 8   기타      12 non-null     int64 \n 9   자연적요인   12 non-null     int64 \n 10  방화      12 non-null     int64 \n 11  방화의심    12 non-null     int64 \n 12  미상      12 non-null     int64 \ndtypes: int64(12), object(1)\nmemory usage: 1.3+ KB\n\n\n\n\n계절 파생변수 추가\n\n봄: 3월, 4월, 5월 여름: 6월, 7월, 8월 가을: 9월, 10월, 11월 겨울: 12월, 1월, 2월\n\n\n\ndata_2020[\"계절\"] = np.where(data_2020[\"항목\"]\\\n                    .isin([\"3월\", \"4월\", \"5월\"]),\"spring\",\n                    np.where(data_2020[\"항목\"]\\\n                    .isin([\"6월\", \"7월\", \"8월\"]),\"summer\",\n                    np.where(data_2020[\"항목\"]\\\n                    .isin([\"9월\", \"10월\", \"11월\"]),\"fall\",\"winter\")))\n\ndata_2021[\"계절\"] = np.where(data_2021[\"항목\"]\\\n                    .isin([\"3월\", \"4월\", \"5월\"]),\"spring\",\n                    np.where(data_2021[\"항목\"].\\\n                    isin([\"6월\", \"7월\", \"8월\"]),\"summer\",\n                    np.where(data_2021[\"항목\"].\\\n                    isin([\"9월\", \"10월\", \"11월\"]),\"fall\",\"winter\")))\n\ndata_2022[\"계절\"] = np.where(data_2022[\"항목\"]\\\n                    .isin([\"3월\", \"4월\", \"5월\"]),\"spring\",\n                    np.where(data_2022[\"항목\"]\\\n                    .isin([\"6월\", \"7월\", \"8월\"]),\"summer\",\n                    np.where(data_2022[\"항목\"]\\\n                    .isin([\"9월\", \"10월\", \"11월\"]),\"fall\",\"winter\")))\n\ndata_2022.head()\n\n\n\n\n\n\n\n\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n계절\n\n\n\n\n0\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n381\nwinter\n\n\n1\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n395\nwinter\n\n\n2\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n301\nspring\n\n\n3\n4월\n3729\n713\n319\n55\n15\n37\n2112\n45\n12\n32\n40\n335\nspring\n\n\n4\n5월\n4105\n687\n287\n82\n14\n33\n2479\n41\n14\n39\n34\n382\nspring\n\n\n\n\n\n\n\n\n\n계절별 화재 횟수 데이터프레임 생성\n\n\nseason_20 = data_2020.groupby('계절').agg(sum2020=('계','sum'))\n\nseason_21 = data_2021.groupby('계절').agg(sum2021=('계','sum'))\n\nseason_22 = data_2022.groupby('계절').agg(sum2022=('계','sum'))\n\nseason_22\n\n\n\n\n\n\n\n\nsum2022\n\n\n계절\n\n\n\n\n\nfall\n8764\n\n\nspring\n11152\n\n\nsummer\n8285\n\n\nwinter\n11912\n\n\n\n\n\n\n\n\n\n데이터 합치기 (‘concat’ 함수이용)\n\n\nseason = pd.concat([season_20,season_21,season_22], axis=1)\nseason\n\n\n\n\n\n\n\n\nsum2020\nsum2021\nsum2022\n\n\n계절\n\n\n\n\n\n\n\nfall\n8962\n7870\n8764\n\n\nspring\n11340\n9498\n11152\n\n\nsummer\n8408\n8099\n8285\n\n\nwinter\n9949\n10800\n11912\n\n\n\n\n\n\n\n\n\n계절 순서를 ‘봄’, ‘여름’, ‘가을’, ’겨울’로 정렬\n\n\nseason = season.loc[['spring', 'summer', 'fall', 'winter']]\nseason\n\n\n\n\n\n\n\n\nsum2020\nsum2021\nsum2022\n\n\n계절\n\n\n\n\n\n\n\nspring\n11340\n9498\n11152\n\n\nsummer\n8408\n8099\n8285\n\n\nfall\n8962\n7870\n8764\n\n\nwinter\n9949\n10800\n11912\n\n\n\n\n\n\n\n\n\n\n&lt;데이터 시각화&gt;\n\n계절별 화재발생 선 그래프 생성\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 6))\nplt.plot(season.index, season['sum2020'], marker='o', label='2020')\nplt.plot(season.index, season['sum2021'], marker='o', label='2021')\nplt.plot(season.index, season['sum2022'], marker='o', label='2022')\n\nplt.legend()\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n&lt;결과 &gt;\n\n2020년, 2021년, 2022년 모두 여름과 가을에 비해봄과 겨울에 화재 발생 빈도가 높게 나타남"
  },
  {
    "objectID": "posts/fire_project/fire_presentation.html#요인별-화재발생-빈도",
    "href": "posts/fire_project/fire_presentation.html#요인별-화재발생-빈도",
    "title": "계절별 화재발생 빈도 및 요인 분석",
    "section": "2. 요인별 화재발생 빈도",
    "text": "2. 요인별 화재발생 빈도\n\n&lt;데이터 전처리&gt;\n\n다시 한번 데이터 확인해보기 \n\n\ndata_2022.head\n\n&lt;bound method NDFrame.head of 0    항목     계  전기적요인  기계적요인  화학적요인  가스누출  교통사고   부주의  기타  자연적요인  방화  방화의심  \\\n0    1월  4098    932    373     52    20    38  2173  48      4  37    31   \n1    2월  4121    778    328     43    12    30  2396  52     11  34    37   \n2    3월  3318    811    308     40     9    26  1694  39      3  46    32   \n3    4월  3729    713    319     55    15    37  2112  45     12  32    40   \n4    5월  4105    687    287     82    14    33  2479  41     14  39    34   \n5    6월  2883    865    275     69    13    30  1230  34     26  36    28   \n6    7월  2706    993    323     67     9    36   902  27     25  35    23   \n7    8월  2696   1089    307     69    14    32   808  43     46  18    28   \n8    9월  2696    765    268     64    13    37  1151  39     50  35    19   \n9   10월  3058    722    329     50     9    39  1529  35      8  33    27   \n10  11월  3010    694    337     55     9    46  1476  48     11  24    24   \n11  12월  3693    962    402     40    14    46  1716  65      4  31    23   \n\n0    미상      계절  \n0   381  winter  \n1   395  winter  \n2   301  spring  \n3   335  spring  \n4   382  spring  \n5   256  summer  \n6   245  summer  \n7   235  summer  \n8   236    fall  \n9   260    fall  \n10  272    fall  \n11  372  winter  &gt;\n\n\n\n\n데이터 합치기 (‘concat’ 함수이용)\n\n\ndata_all = pd.concat([data_2020, data_2021,data_2022])\ndata_all\n\n\n\n\n\n\n\n\n항목\n계\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n계절\n\n\n\n\n0\n1월\n3100\n781\n377\n43\n17\n34\n1453\n44\n2\n33\n29\n287\nwinter\n\n\n1\n2월\n3011\n748\n342\n46\n15\n40\n1465\n35\n2\n41\n33\n244\nwinter\n\n\n2\n3월\n4053\n702\n343\n56\n12\n34\n2443\n55\n13\n40\n44\n311\nspring\n\n\n3\n4월\n4381\n648\n304\n57\n10\n41\n2849\n55\n8\n28\n50\n331\nspring\n\n\n4\n5월\n2906\n704\n287\n52\n13\n33\n1462\n39\n14\n30\n27\n245\nspring\n\n\n5\n6월\n2968\n732\n312\n60\n9\n47\n1435\n38\n16\n36\n27\n256\nsummer\n\n\n6\n7월\n2423\n861\n289\n57\n12\n35\n885\n29\n30\n24\n28\n173\nsummer\n\n\n7\n8월\n3017\n1331\n355\n57\n7\n31\n834\n52\n71\n24\n22\n233\nsummer\n\n\n8\n9월\n2513\n730\n299\n45\n5\n40\n1010\n48\n61\n24\n22\n229\nfall\n\n\n9\n10월\n3287\n608\n341\n53\n11\n49\n1775\n68\n6\n37\n29\n310\nfall\n\n\n10\n11월\n3162\n669\n369\n46\n17\n34\n1608\n56\n11\n25\n41\n286\nfall\n\n\n11\n12월\n3838\n815\n435\n58\n13\n40\n1967\n81\n4\n35\n29\n361\nwinter\n\n\n0\n1월\n3847\n1067\n475\n48\n28\n36\n1705\n61\n7\n33\n30\n357\nwinter\n\n\n1\n2월\n3500\n708\n322\n52\n13\n37\n1943\n62\n7\n27\n40\n289\nwinter\n\n\n2\n3월\n3180\n721\n348\n61\n14\n26\n1637\n64\n10\n29\n26\n244\nspring\n\n\n3\n4월\n3603\n693\n339\n60\n15\n32\n2037\n67\n6\n21\n37\n296\nspring\n\n\n4\n5월\n2715\n679\n305\n54\n9\n35\n1249\n65\n26\n28\n34\n231\nspring\n\n\n5\n6월\n2456\n666\n324\n54\n9\n28\n1076\n48\n14\n19\n26\n192\nsummer\n\n\n6\n7월\n3036\n1122\n328\n89\n7\n34\n1075\n45\n65\n23\n16\n232\nsummer\n\n\n7\n8월\n2607\n926\n315\n76\n11\n30\n905\n43\n46\n17\n25\n213\nsummer\n\n\n8\n9월\n2252\n643\n239\n44\n5\n31\n983\n48\n28\n23\n18\n190\nfall\n\n\n9\n10월\n2710\n658\n326\n50\n8\n31\n1260\n75\n13\n29\n27\n233\nfall\n\n\n10\n11월\n2908\n720\n335\n39\n9\n37\n1379\n45\n10\n33\n24\n277\nfall\n\n\n11\n12월\n3453\n869\n382\n56\n18\n41\n1626\n56\n9\n26\n36\n334\nwinter\n\n\n0\n1월\n4098\n932\n373\n52\n20\n38\n2173\n48\n4\n37\n31\n381\nwinter\n\n\n1\n2월\n4121\n778\n328\n43\n12\n30\n2396\n52\n11\n34\n37\n395\nwinter\n\n\n2\n3월\n3318\n811\n308\n40\n9\n26\n1694\n39\n3\n46\n32\n301\nspring\n\n\n3\n4월\n3729\n713\n319\n55\n15\n37\n2112\n45\n12\n32\n40\n335\nspring\n\n\n4\n5월\n4105\n687\n287\n82\n14\n33\n2479\n41\n14\n39\n34\n382\nspring\n\n\n5\n6월\n2883\n865\n275\n69\n13\n30\n1230\n34\n26\n36\n28\n256\nsummer\n\n\n6\n7월\n2706\n993\n323\n67\n9\n36\n902\n27\n25\n35\n23\n245\nsummer\n\n\n7\n8월\n2696\n1089\n307\n69\n14\n32\n808\n43\n46\n18\n28\n235\nsummer\n\n\n8\n9월\n2696\n765\n268\n64\n13\n37\n1151\n39\n50\n35\n19\n236\nfall\n\n\n9\n10월\n3058\n722\n329\n50\n9\n39\n1529\n35\n8\n33\n27\n260\nfall\n\n\n10\n11월\n3010\n694\n337\n55\n9\n46\n1476\n48\n11\n24\n24\n272\nfall\n\n\n11\n12월\n3693\n962\n402\n40\n14\n46\n1716\n65\n4\n31\n23\n372\nwinter\n\n\n\n\n\n\n\n\n\n필요없는 열 제거\n\n\ndata_all = data_all.drop(columns=['계'])\ndata_all.head(3)\n\n\n\n\n\n\n\n\n항목\n전기적요인\n기계적요인\n화학적요인\n가스누출\n교통사고\n부주의\n기타\n자연적요인\n방화\n방화의심\n미상\n계절\n\n\n\n\n0\n1월\n781\n377\n43\n17\n34\n1453\n44\n2\n33\n29\n287\nwinter\n\n\n1\n2월\n748\n342\n46\n15\n40\n1465\n35\n2\n41\n33\n244\nwinter\n\n\n2\n3월\n702\n343\n56\n12\n34\n2443\n55\n13\n40\n44\n311\nspring\n\n\n\n\n\n\n\n\n\n요인별 평균 내기\n\n\n# 행과 열 바꿔줌\ndata_all = data_all.transpose()\ndata_all.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n항목\n1월\n2월\n3월\n4월\n5월\n6월\n7월\n8월\n9월\n10월\n...\n3월\n4월\n5월\n6월\n7월\n8월\n9월\n10월\n11월\n12월\n\n\n전기적요인\n781\n748\n702\n648\n704\n732\n861\n1331\n730\n608\n...\n811\n713\n687\n865\n993\n1089\n765\n722\n694\n962\n\n\n기계적요인\n377\n342\n343\n304\n287\n312\n289\n355\n299\n341\n...\n308\n319\n287\n275\n323\n307\n268\n329\n337\n402\n\n\n화학적요인\n43\n46\n56\n57\n52\n60\n57\n57\n45\n53\n...\n40\n55\n82\n69\n67\n69\n64\n50\n55\n40\n\n\n가스누출\n17\n15\n12\n10\n13\n9\n12\n7\n5\n11\n...\n9\n15\n14\n13\n9\n14\n13\n9\n9\n14\n\n\n\n\n5 rows × 36 columns\n\n\n\n\n# 필요없는 열 제거\ndata_all = data_all.drop(\"항목\", axis=0)\ndata_all = data_all.drop(\"계절\", axis=0)\ndata_all.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n전기적요인\n781\n748\n702\n648\n704\n732\n861\n1331\n730\n608\n...\n811\n713\n687\n865\n993\n1089\n765\n722\n694\n962\n\n\n기계적요인\n377\n342\n343\n304\n287\n312\n289\n355\n299\n341\n...\n308\n319\n287\n275\n323\n307\n268\n329\n337\n402\n\n\n화학적요인\n43\n46\n56\n57\n52\n60\n57\n57\n45\n53\n...\n40\n55\n82\n69\n67\n69\n64\n50\n55\n40\n\n\n가스누출\n17\n15\n12\n10\n13\n9\n12\n7\n5\n11\n...\n9\n15\n14\n13\n9\n14\n13\n9\n9\n14\n\n\n교통사고\n34\n40\n34\n41\n33\n47\n35\n31\n40\n49\n...\n26\n37\n33\n30\n36\n32\n37\n39\n46\n46\n\n\n\n\n5 rows × 36 columns\n\n\n\n\n# 데이터타입이 object로 되어 있음\ndata_all.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 11 entries, 전기적요인 to 미상\nData columns (total 36 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   0       11 non-null     object\n 1   1       11 non-null     object\n 2   2       11 non-null     object\n 3   3       11 non-null     object\n 4   4       11 non-null     object\n 5   5       11 non-null     object\n 6   6       11 non-null     object\n 7   7       11 non-null     object\n 8   8       11 non-null     object\n 9   9       11 non-null     object\n 10  10      11 non-null     object\n 11  11      11 non-null     object\n 12  0       11 non-null     object\n 13  1       11 non-null     object\n 14  2       11 non-null     object\n 15  3       11 non-null     object\n 16  4       11 non-null     object\n 17  5       11 non-null     object\n 18  6       11 non-null     object\n 19  7       11 non-null     object\n 20  8       11 non-null     object\n 21  9       11 non-null     object\n 22  10      11 non-null     object\n 23  11      11 non-null     object\n 24  0       11 non-null     object\n 25  1       11 non-null     object\n 26  2       11 non-null     object\n 27  3       11 non-null     object\n 28  4       11 non-null     object\n 29  5       11 non-null     object\n 30  6       11 non-null     object\n 31  7       11 non-null     object\n 32  8       11 non-null     object\n 33  9       11 non-null     object\n 34  10      11 non-null     object\n 35  11      11 non-null     object\ndtypes: object(36)\nmemory usage: 3.2+ KB\n\n\n\n\n# 데이터타입을 int로 변경\ndata_all=data_all.astype(int)\ndata_all.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 11 entries, 전기적요인 to 미상\nData columns (total 36 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   0       11 non-null     int32\n 1   1       11 non-null     int32\n 2   2       11 non-null     int32\n 3   3       11 non-null     int32\n 4   4       11 non-null     int32\n 5   5       11 non-null     int32\n 6   6       11 non-null     int32\n 7   7       11 non-null     int32\n 8   8       11 non-null     int32\n 9   9       11 non-null     int32\n 10  10      11 non-null     int32\n 11  11      11 non-null     int32\n 12  0       11 non-null     int32\n 13  1       11 non-null     int32\n 14  2       11 non-null     int32\n 15  3       11 non-null     int32\n 16  4       11 non-null     int32\n 17  5       11 non-null     int32\n 18  6       11 non-null     int32\n 19  7       11 non-null     int32\n 20  8       11 non-null     int32\n 21  9       11 non-null     int32\n 22  10      11 non-null     int32\n 23  11      11 non-null     int32\n 24  0       11 non-null     int32\n 25  1       11 non-null     int32\n 26  2       11 non-null     int32\n 27  3       11 non-null     int32\n 28  4       11 non-null     int32\n 29  5       11 non-null     int32\n 30  6       11 non-null     int32\n 31  7       11 non-null     int32\n 32  8       11 non-null     int32\n 33  9       11 non-null     int32\n 34  10      11 non-null     int32\n 35  11      11 non-null     int32\ndtypes: int32(36)\nmemory usage: 1.6+ KB\n\n\n\n\n1년동안 일어나는 요인별 화재건수를 ’total’열 생성(3년 평균)\n\n\ndata_all[\"total\"] = data_all.sum(axis=1)/3\ndata_all.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n3\n4\n5\n6\n7\n8\n9\n10\n11\ntotal\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n전기적요인\n781\n748\n702\n648\n704\n732\n861\n1331\n730\n608\n...\n713\n687\n865\n993\n1089\n765\n722\n694\n962\n9604.000000\n\n\n기계적요인\n377\n342\n343\n304\n287\n312\n289\n355\n299\n341\n...\n319\n287\n275\n323\n307\n268\n329\n337\n402\n3982.333333\n\n\n화학적요인\n43\n46\n56\n57\n52\n60\n57\n57\n45\n53\n...\n55\n82\n69\n67\n69\n64\n50\n55\n40\n666.333333\n\n\n가스누출\n17\n15\n12\n10\n13\n9\n12\n7\n5\n11\n...\n15\n14\n13\n9\n14\n13\n9\n9\n14\n146.000000\n\n\n교통사고\n34\n40\n34\n41\n33\n47\n35\n31\n40\n49\n...\n37\n33\n30\n36\n32\n37\n39\n46\n46\n428.666667\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n## 한글\nfrom matplotlib import font_manager, rc\n\n# 한글 폰트 설정\nfont_path = \"C:/Windows/Fonts/malgun.ttf\"  # 예시: 윈도우 시스템에 있는 맑은 고딕 폰트 경로\nfont_name = font_manager.FontProperties(fname=font_path).get_name()\nrc('font', family=font_name)\n\n\n\n&lt;데이터 시각화&gt;\n\n요인별 화재건수\n\n\ndata_all[\"total\"].plot.bar(rot=0)\nplt.xticks(fontsize=7, rotation=45)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n&lt;결과&gt;\n\n3년간 화재발생 요인의 평균을 분석했을 때 부주의가 가장 높게 나타남"
  },
  {
    "objectID": "posts/fire_project/fire_presentation.html#요인별-인명피해",
    "href": "posts/fire_project/fire_presentation.html#요인별-인명피해",
    "title": "계절별 화재발생 빈도 및 요인 분석",
    "section": "3. 요인별 인명피해",
    "text": "3. 요인별 인명피해\n\n&lt;데이터 설명&gt;\n\ndamage = pd.read_csv(\"di.csv\")\ndamage\n\n\n\n\n\n\n\n\n항목\n2020\n2020.1\n2020.2\n2020.3\n2020.4\n2020.5\n2020.6\n2020.7\n2020.8\n...\n2022.16\n2022.17\n2022.18\n2022.19\n2022.20\n2022.21\n2022.22\n2022.23\n2022.24\n2022.25\n\n\n\n\n0\n항목\n계\n계\n전기적요인\n전기적요인\n기계적요인\n기계적요인\n화학적요인\n화학적요인\n가스누출\n...\n자연적요인\n자연적요인\n방화\n방화\n방화의심\n방화의심\n제품결함\n제품결함\n미상\n미상\n\n\n1\n항목\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n...\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n\n\n2\n합계\n365\n1918\n39\n359\n4\n126\n11\n64\n11\n...\n0\n5\n43\n78\n35\n53\n1\n18\n107\n505\n\n\n3\n1월\n44\n144\n9\n30\n0\n7\n0\n5\n6\n...\n0\n0\n3\n7\n2\n3\n0\n0\n12\n47\n\n\n4\n2월\n28\n150\n2\n16\n0\n13\n0\n1\n0\n...\n0\n0\n0\n9\n4\n4\n0\n1\n12\n64\n\n\n5\n3월\n40\n192\n7\n33\n1\n10\n0\n10\n0\n...\n0\n1\n4\n9\n6\n10\n0\n0\n9\n35\n\n\n6\n4월\n72\n197\n5\n60\n1\n5\n0\n6\n0\n...\n0\n1\n8\n6\n0\n2\n0\n0\n10\n50\n\n\n7\n5월\n22\n127\n2\n25\n0\n7\n1\n4\n1\n...\n0\n0\n1\n6\n1\n2\n1\n0\n12\n31\n\n\n8\n6월\n19\n110\n1\n23\n0\n3\n1\n0\n2\n...\n0\n0\n11\n8\n4\n3\n0\n2\n9\n24\n\n\n9\n7월\n28\n180\n3\n12\n0\n25\n0\n6\n1\n...\n0\n0\n2\n6\n4\n15\n0\n2\n3\n28\n\n\n10\n8월\n8\n150\n2\n46\n0\n17\n1\n2\n0\n...\n0\n1\n2\n7\n4\n4\n0\n2\n4\n25\n\n\n11\n9월\n14\n123\n2\n36\n0\n5\n2\n6\n0\n...\n0\n1\n3\n7\n2\n3\n0\n3\n4\n32\n\n\n12\n10월\n25\n161\n2\n24\n1\n11\n1\n8\n0\n...\n0\n1\n2\n3\n5\n3\n0\n4\n8\n17\n\n\n13\n11월\n31\n183\n2\n13\n0\n13\n4\n14\n0\n...\n0\n0\n2\n5\n0\n0\n0\n0\n4\n38\n\n\n14\n12월\n34\n201\n2\n41\n1\n10\n1\n2\n1\n...\n0\n0\n5\n5\n3\n4\n0\n4\n20\n114\n\n\n\n\n15 rows × 75 columns\n\n\n\n\n\n\n&lt;데이터 전처리&gt;\n\n연도별로 분리\n\n\ndamage_20 = damage[['항목'] + damage.filter(like='2020').columns.tolist()]\ndamage_21 = damage[['항목'] + damage.filter(like='2021').columns.tolist()]\ndamage_22 = damage[['항목'] + damage.filter(like='2022').columns.tolist()]\n\ndamage_22.head()\n\n\n\n\n\n\n\n\n항목\n2022\n2022.1\n2022.2\n2022.3\n2022.4\n2022.5\n2022.6\n2022.7\n2022.8\n...\n2022.16\n2022.17\n2022.18\n2022.19\n2022.20\n2022.21\n2022.22\n2022.23\n2022.24\n2022.25\n\n\n\n\n0\n항목\n계\n계\n전기적요인\n전기적요인\n기계적요인\n기계적요인\n화학적요인\n화학적요인\n가스누출\n...\n자연적요인\n자연적요인\n방화\n방화\n방화의심\n방화의심\n제품결함\n제품결함\n미상\n미상\n\n\n1\n항목\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n...\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n\n\n2\n합계\n341\n2327\n48\n410\n2\n102\n6\n81\n6\n...\n0\n5\n43\n78\n35\n53\n1\n18\n107\n505\n\n\n3\n1월\n41\n229\n6\n38\n0\n6\n0\n5\n2\n...\n0\n0\n3\n7\n2\n3\n0\n0\n12\n47\n\n\n4\n2월\n28\n230\n2\n24\n0\n18\n0\n3\n0\n...\n0\n0\n0\n9\n4\n4\n0\n1\n12\n64\n\n\n\n\n5 rows × 27 columns\n\n\n\n&lt;br&gt;\n\n행 올리기 (반복되는 기존 열 삭제 및 첫번째 행 올리기)\n\n\ndamage_20.columns = damage_20.iloc[0]\ndamage_20 = damage_20[1:3]\ndamage_20 = damage_20.reset_index(drop=True)\ndamage_20 = damage_20.drop(columns=['항목','계'])\n\ndamage_21.columns = damage_21.iloc[0]\ndamage_21 = damage_21[1:3]\ndamage_21 = damage_21.reset_index(drop=True)\ndamage_21 = damage_21.drop(columns=['항목','계'])\n\ndamage_22.columns = damage_22.iloc[0]\ndamage_22 = damage_22[1:3]\ndamage_22 = damage_22.reset_index(drop=True)\ndamage_22 = damage_22.drop(columns=['항목', '계', '제품결함'])\n\ndamage_22\n\n\n\n\n\n\n\n\n전기적요인\n전기적요인\n기계적요인\n기계적요인\n화학적요인\n화학적요인\n가스누출\n가스누출\n교통사고\n교통사고\n...\n기타\n기타\n자연적요인\n자연적요인\n방화\n방화\n방화의심\n방화의심\n미상\n미상\n\n\n\n\n0\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n...\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n사망\n부상\n\n\n1\n48\n410\n2\n102\n6\n81\n6\n141\n9\n19\n...\n10\n20\n0\n5\n43\n78\n35\n53\n107\n505\n\n\n\n\n2 rows × 22 columns\n\n\n\n\n\n행열 전환\n\n\ndamage_20 = damage_20.transpose()\n\ndamage_21 = damage_21.transpose()\n\ndamage_22 = damage_22.transpose()\n\ndamage_22.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n0\n\n\n\n\n\n\n전기적요인\n사망\n48\n\n\n전기적요인\n부상\n410\n\n\n기계적요인\n사망\n2\n\n\n기계적요인\n부상\n102\n\n\n화학적요인\n사망\n6\n\n\n\n\n\n\n\n\n\n데이터 합치기 (‘concat’ 함수이용)\n\n\ndamage_total = pd.concat([damage_20, damage_21[1], damage_22[1]], axis=1)\n    \ndamage_total = damage_total.drop(0, axis=1)\n    \ndamage_total.head()\n\n\n\n\n\n\n\n\n1\n1\n1\n\n\n0\n\n\n\n\n\n\n\n전기적요인\n39\n42\n48\n\n\n전기적요인\n359\n276\n410\n\n\n기계적요인\n4\n4\n2\n\n\n기계적요인\n126\n105\n102\n\n\n화학적요인\n11\n2\n6\n\n\n\n\n\n\n\n\n\n계산 가능한 숫자형 데이터로 전환하기\n\n\ndamage_total=damage_total.astype(int)\ndamage_total.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 22 entries, 전기적요인 to 미상\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   1       22 non-null     int32\n 1   1       22 non-null     int32\n 2   1       22 non-null     int32\ndtypes: int32(3)\nmemory usage: 440.0+ bytes\n\n\n\n\n사망 데이터 분류하기\n\n\ndeath = damage_total.iloc[::2]\ndeath\n\n\n\n\n\n\n\n\n1\n1\n1\n\n\n0\n\n\n\n\n\n\n\n전기적요인\n39\n42\n48\n\n\n기계적요인\n4\n4\n2\n\n\n화학적요인\n11\n2\n6\n\n\n가스누출\n11\n1\n6\n\n\n교통사고\n14\n13\n9\n\n\n부주의\n77\n67\n74\n\n\n기타\n2\n2\n10\n\n\n자연적요인\n0\n0\n0\n\n\n방화\n27\n25\n43\n\n\n방화의심\n39\n23\n35\n\n\n미상\n141\n97\n107\n\n\n\n\n\n\n\n\n\n부상 데이터 분류하기\n\n\ninjury = damage_total.iloc[1::2]\ninjury\n\n\n\n\n\n\n\n\n1\n1\n1\n\n\n0\n\n\n\n\n\n\n\n전기적요인\n359\n276\n410\n\n\n기계적요인\n126\n105\n102\n\n\n화학적요인\n64\n79\n81\n\n\n가스누출\n83\n78\n141\n\n\n교통사고\n6\n17\n19\n\n\n부주의\n764\n760\n895\n\n\n기타\n39\n31\n20\n\n\n자연적요인\n2\n2\n5\n\n\n방화\n78\n77\n78\n\n\n방화의심\n61\n63\n53\n\n\n미상\n336\n366\n505\n\n\n\n\n\n\n\n\n\n사망,부상별 합계 ,평균 변수 및 백분율 변수 추가하기\n\n\ndeath_all = death.assign(total = death.sum(axis = 1),\n                         mean = lambda x : x[\"total\"] / 3 ,\n                         ratio = lambda x : (x[\"mean\"] / data_all[\"total\"]) * 100)\n\ninjury_all = injury.assign(total = injury.sum(axis = 1),\n                           mean = lambda x : x[\"total\"] / 3,\n                           ratio = lambda x : (x[\"mean\"] / data_all[\"total\"]) * 100)\n             \ninjury_all.head()\n\n\n\n\n\n\n\n\n1\n1\n1\ntotal\nmean\nratio\n\n\n0\n\n\n\n\n\n\n\n\n\n\n전기적요인\n359\n276\n410\n1045\n348.333333\n3.626961\n\n\n기계적요인\n126\n105\n102\n333\n111.000000\n2.787311\n\n\n화학적요인\n64\n79\n81\n224\n74.666667\n11.205603\n\n\n가스누출\n83\n78\n141\n302\n100.666667\n68.949772\n\n\n교통사고\n6\n17\n19\n42\n14.000000\n3.265941\n\n\n\n\n\n\n\n\n\n\n&lt;데이터 시각화&gt;\n\n총 화재 발생 대비 사망률 막대 그래프 생성\n\n\ndeath_all[\"ratio\"].plot.bar(rot=0)\nplt.xticks(fontsize=7, rotation=20)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n(연간 사망 횟수 평균/연간 화재 발생 횟수 평균*100) [%]\n\n\n\n총 화재 발생 대비 부상률 막대 그래프 생성\n\n\ninjury_all[\"ratio\"].plot.bar(rot=0)\nplt.xticks(fontsize=7, rotation=20)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n(연간 부상 횟수 평균/연간 화재 발생 횟수 평균*100) [%]\n\n\n\n\n&lt;결과&gt;\n\n사망률은 방화와 방화 의심, 부상률은 가스누출이 높게 나타남"
  },
  {
    "objectID": "posts/fire_project/fire_presentation.html#결론",
    "href": "posts/fire_project/fire_presentation.html#결론",
    "title": "계절별 화재발생 빈도 및 요인 분석",
    "section": "4. 결론",
    "text": "4. 결론\n\n건조한 날씨와 난방기구 사용으로 화재가 많이 발생하는 봄과 겨울에 특히 주의가 필요함\n화재 발생의 주요 원인이 부주의인 점을 감안할 때, 화재 예방을 위한 대중 교육과 캠페인이 중요함\n방화 및 방화 의심 화재에 의한 사망이 많으므로 고의적인 화재에 대한 강력한 법적 대응과 예방책을 마련해야 함\n가스 누출 화재에 의한 부상이 많으므로 가스 안전 관리와 관련된 법규와 교육을 강화할 필요가 있음\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW1",
    "section": "",
    "text": "다음 표의 내용을 데이터 프레임으로 만들어 출력해 보세요. (p.85)\n\nimport pandas as pd\ndf=pd.DataFrame({\n    '제품':['사과','딸기','수박'],\n    '가격':[1800,1500,3000],\n    '판매량':[24,38,13]\n})\nprint(df)\n\n   제품    가격  판매량\n0  사과  1800   24\n1  딸기  1500   38\n2  수박  3000   13\n\n\nmpg데이터 변수명 변경 (p.115)\n\nmpg=pd.read_csv('../../data/mpg.csv')\nmpg_new=mpg.copy()\nmpg_new=mpg_new.rename(columns={'cty':'city'})\nmpg_new=mpg_new.rename(columns={'hwy':'highway'})\nprint(mpg_new)\n\n    manufacturer   model  displ  year  cyl       trans drv  city  highway fl  \\\n0           audi      a4    1.8  1999    4    auto(l5)   f    18       29  p   \n1           audi      a4    1.8  1999    4  manual(m5)   f    21       29  p   \n2           audi      a4    2.0  2008    4  manual(m6)   f    20       31  p   \n3           audi      a4    2.0  2008    4    auto(av)   f    21       30  p   \n4           audi      a4    2.8  1999    6    auto(l5)   f    16       26  p   \n..           ...     ...    ...   ...  ...         ...  ..   ...      ... ..   \n229   volkswagen  passat    2.0  2008    4    auto(s6)   f    19       28  p   \n230   volkswagen  passat    2.0  2008    4  manual(m6)   f    21       29  p   \n231   volkswagen  passat    2.8  1999    6    auto(l5)   f    16       26  p   \n232   volkswagen  passat    2.8  1999    6  manual(m5)   f    18       26  p   \n233   volkswagen  passat    3.6  2008    6    auto(s6)   f    17       26  p   \n\n    category  \n0    compact  \n1    compact  \n2    compact  \n3    compact  \n4    compact  \n..       ...  \n229  midsize  \n230  midsize  \n231  midsize  \n232  midsize  \n233  midsize  \n\n[234 rows x 11 columns]\n\n\nmidway.csv를 불러와 데이터의 특징을 파악하세요. (p.130)\n\nmdw=pd.read_csv('../../data/midwest.csv')\nmdw.describe()\n\n\n\n\n\n\n\n\nPID\narea\npoptotal\npopdensity\npopwhite\npopblack\npopamerindian\npopasian\npopother\npercwhite\n...\nperchsd\npercollege\npercprof\npoppovertyknown\npercpovertyknown\npercbelowpoverty\npercchildbelowpovert\npercadultpoverty\npercelderlypoverty\ninmetro\n\n\n\n\ncount\n437.000000\n437.000000\n4.370000e+02\n437.000000\n4.370000e+02\n4.370000e+02\n437.000000\n437.000000\n437.000000\n437.000000\n...\n437.000000\n437.000000\n437.000000\n4.370000e+02\n437.000000\n437.000000\n437.000000\n437.000000\n437.000000\n437.000000\n\n\nmean\n1437.338673\n0.033169\n9.613030e+04\n3097.742985\n8.183992e+04\n1.102388e+04\n343.109840\n1310.464531\n1612.931350\n95.558441\n...\n73.965546\n18.272736\n4.447259\n9.364228e+04\n97.110267\n12.510505\n16.447464\n10.918798\n11.389043\n0.343249\n\n\nstd\n876.390266\n0.014679\n2.981705e+05\n7664.751786\n2.001966e+05\n7.895827e+04\n868.926751\n9518.394189\n18526.540699\n7.087358\n...\n5.843177\n6.261908\n2.408427\n2.932351e+05\n2.749863\n5.150155\n7.228634\n5.109166\n3.661259\n0.475338\n\n\nmin\n561.000000\n0.005000\n1.701000e+03\n85.050000\n4.160000e+02\n0.000000e+00\n4.000000\n0.000000\n0.000000\n10.694087\n...\n46.912261\n7.336108\n0.520291\n1.696000e+03\n80.902441\n2.180168\n1.918955\n1.938504\n3.547067\n0.000000\n\n\n25%\n670.000000\n0.024000\n1.884000e+04\n622.407407\n1.863000e+04\n2.900000e+01\n44.000000\n35.000000\n20.000000\n94.886032\n...\n71.325329\n14.113725\n2.997957\n1.836400e+04\n96.894572\n9.198715\n11.624088\n7.668009\n8.911763\n0.000000\n\n\n50%\n1221.000000\n0.030000\n3.532400e+04\n1156.208330\n3.447100e+04\n2.010000e+02\n94.000000\n102.000000\n66.000000\n98.032742\n...\n74.246891\n16.797562\n3.814239\n3.378800e+04\n98.169562\n11.822313\n15.270164\n10.007610\n10.869119\n0.000000\n\n\n75%\n2059.000000\n0.038000\n7.565100e+04\n2330.000000\n7.296800e+04\n1.291000e+03\n288.000000\n401.000000\n345.000000\n99.074935\n...\n77.195345\n20.549893\n4.949324\n7.284000e+04\n98.598636\n15.133226\n20.351878\n13.182182\n13.412162\n1.000000\n\n\nmax\n3052.000000\n0.110000\n5.105067e+06\n88018.396600\n3.204947e+06\n1.317147e+06\n10289.000000\n188565.000000\n384119.000000\n99.822821\n...\n88.898674\n48.078510\n20.791321\n5.023523e+06\n99.860384\n48.691099\n64.308477\n43.312464\n31.161972\n1.000000\n\n\n\n\n8 rows × 25 columns\n\n\n\npoptotal(전체 인구) 변수를 total로, popasian(아시아 인구) 변수를 asian으로 수정하세요.\n\nmdw=mdw.rename(columns={'poptotal':'total'})\nmdw=mdw.rename(columns={'popasian':'asian'})\nprint(mdw[['total','asian']])\n\n      total  asian\n0     66090    249\n1     10626     48\n2     14991     16\n3     30806    150\n4      5836      5\n..      ...    ...\n432  304715   2699\n433   46104     92\n434   19385     43\n435  140320   1728\n436   73605    722\n\n[437 rows x 2 columns]\n\n\ntotal,asian 변수를 이용해 ‘전체 인구 대비 아시아 인구 백분율’ 파생변수를 추가하고, 히스토그램을 만들어 분포를 살펴보세요.\n\nimport matplotlib.pyplot as plt\nmdw['per_asian']=mdw['asian']/mdw['total']*100\nprint(mdw)\nmdw['per_asian'].hist()\nplt.show()\n\n      PID     county state   area   total   popdensity  popwhite  popblack  \\\n0     561      ADAMS    IL  0.052   66090  1270.961540     63917      1702   \n1     562  ALEXANDER    IL  0.014   10626   759.000000      7054      3496   \n2     563       BOND    IL  0.022   14991   681.409091     14477       429   \n3     564      BOONE    IL  0.017   30806  1812.117650     29344       127   \n4     565      BROWN    IL  0.018    5836   324.222222      5264       547   \n..    ...        ...   ...    ...     ...          ...       ...       ...   \n432  3048   WAUKESHA    WI  0.034  304715  8962.205880    298313      1096   \n433  3049    WAUPACA    WI  0.045   46104  1024.533330     45695        22   \n434  3050   WAUSHARA    WI  0.037   19385   523.918919     19094        29   \n435  3051  WINNEBAGO    WI  0.035  140320  4009.142860    136822       697   \n436  3052       WOOD    WI  0.048   73605  1533.437500     72157        90   \n\n     popamerindian  asian  ...  percprof  poppovertyknown  percpovertyknown  \\\n0               98    249  ...  4.355859            63628         96.274777   \n1               19     48  ...  2.870315            10529         99.087145   \n2               35     16  ...  4.488572            14235         94.956974   \n3               46    150  ...  4.197800            30337         98.477569   \n4               14      5  ...  3.367680             4815         82.505140   \n..             ...    ...  ...       ...              ...               ...   \n432            672   2699  ...  7.667090           299802         98.387674   \n433            125     92  ...  3.138596            44412         96.330036   \n434             70     43  ...  2.620907            19163         98.854785   \n435            685   1728  ...  5.659847           133950         95.460376   \n436            481    722  ...  4.583725            72685         98.750085   \n\n     percbelowpoverty  percchildbelowpovert  percadultpoverty  \\\n0           13.151443             18.011717         11.009776   \n1           32.244278             45.826514         27.385647   \n2           12.068844             14.036061         10.852090   \n3            7.209019             11.179536          5.536013   \n4           13.520249             13.022889         11.143211   \n..                ...                   ...               ...   \n432          3.121060              3.785820          2.590061   \n433          8.488697             10.071411          6.953799   \n434         13.786985             20.050708         11.695784   \n435          8.804031             10.592031          8.660587   \n436          8.525831             11.162997          7.375656   \n\n     percelderlypoverty  inmetro  category  per_asian  \n0             12.443812        0       AAR   0.376759  \n1             25.228976        0       LHR   0.451722  \n2             12.697410        0       AAR   0.106731  \n3              6.217047        1       ALU   0.486918  \n4             19.200000        0       AAR   0.085675  \n..                  ...      ...       ...        ...  \n432            4.085479        1       HLU   0.885746  \n433           10.338641        0       AAR   0.199549  \n434           11.804558        0       AAR   0.221821  \n435            6.661094        1       HAU   1.231471  \n436            7.882918        0       AAR   0.980912  \n\n[437 rows x 29 columns]\n\n\n\n\n\n\n\n\n\n아시아 인구 백분율 전체 평균을 구하고, 평균을 초과하면 ‘Large’, 그 외에는 ’Small’을 부여한 파생변수를 만들어 보세요.\n\nimport numpy as np\navg_asian=mdw['per_asian'].mean()\nmdw['amo_asian']=np.where((mdw['per_asian']&gt;avg_asian),'Large','Small')\nprint(mdw)\n\n      PID     county state   area   total   popdensity  popwhite  popblack  \\\n0     561      ADAMS    IL  0.052   66090  1270.961540     63917      1702   \n1     562  ALEXANDER    IL  0.014   10626   759.000000      7054      3496   \n2     563       BOND    IL  0.022   14991   681.409091     14477       429   \n3     564      BOONE    IL  0.017   30806  1812.117650     29344       127   \n4     565      BROWN    IL  0.018    5836   324.222222      5264       547   \n..    ...        ...   ...    ...     ...          ...       ...       ...   \n432  3048   WAUKESHA    WI  0.034  304715  8962.205880    298313      1096   \n433  3049    WAUPACA    WI  0.045   46104  1024.533330     45695        22   \n434  3050   WAUSHARA    WI  0.037   19385   523.918919     19094        29   \n435  3051  WINNEBAGO    WI  0.035  140320  4009.142860    136822       697   \n436  3052       WOOD    WI  0.048   73605  1533.437500     72157        90   \n\n     popamerindian  asian  ...  poppovertyknown  percpovertyknown  \\\n0               98    249  ...            63628         96.274777   \n1               19     48  ...            10529         99.087145   \n2               35     16  ...            14235         94.956974   \n3               46    150  ...            30337         98.477569   \n4               14      5  ...             4815         82.505140   \n..             ...    ...  ...              ...               ...   \n432            672   2699  ...           299802         98.387674   \n433            125     92  ...            44412         96.330036   \n434             70     43  ...            19163         98.854785   \n435            685   1728  ...           133950         95.460376   \n436            481    722  ...            72685         98.750085   \n\n     percbelowpoverty  percchildbelowpovert  percadultpoverty  \\\n0           13.151443             18.011717         11.009776   \n1           32.244278             45.826514         27.385647   \n2           12.068844             14.036061         10.852090   \n3            7.209019             11.179536          5.536013   \n4           13.520249             13.022889         11.143211   \n..                ...                   ...               ...   \n432          3.121060              3.785820          2.590061   \n433          8.488697             10.071411          6.953799   \n434         13.786985             20.050708         11.695784   \n435          8.804031             10.592031          8.660587   \n436          8.525831             11.162997          7.375656   \n\n     percelderlypoverty  inmetro  category  per_asian  amo_asian  \n0             12.443812        0       AAR   0.376759      Small  \n1             25.228976        0       LHR   0.451722      Small  \n2             12.697410        0       AAR   0.106731      Small  \n3              6.217047        1       ALU   0.486918      Small  \n4             19.200000        0       AAR   0.085675      Small  \n..                  ...      ...       ...        ...        ...  \n432            4.085479        1       HLU   0.885746      Large  \n433           10.338641        0       AAR   0.199549      Small  \n434           11.804558        0       AAR   0.221821      Small  \n435            6.661094        1       HAU   1.231471      Large  \n436            7.882918        0       AAR   0.980912      Large  \n\n[437 rows x 30 columns]\n\n\n’Large’와 ’Small’에 해당하는 지역이 얼마나 많은지 빈도표와 빈도 막대 그래프를 만들어 확인해 보세요.\n\nplt.clf()\nasian_counts=mdw['amo_asian'].value_counts()\nprint(asian_counts)\nasian_counts.plot.bar(rot=0)\nplt.show()\n\namo_asian\nSmall    318\nLarge    119\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw2.1/index.html",
    "href": "posts/hw2.1/index.html",
    "title": "HW2.1",
    "section": "",
    "text": "exam데이터 불러오기\n\nimport pandas as pd\nimport plotly.express as px\nexam=pd.read_csv('../../data/exam.csv')\nexam\n\n\n\n\n\n\n\n\nid\nnclass\nmath\nenglish\nscience\n\n\n\n\n0\n1\n1\n50\n98\n50\n\n\n1\n2\n1\n60\n97\n60\n\n\n2\n3\n1\n45\n86\n78\n\n\n3\n4\n1\n30\n98\n58\n\n\n4\n5\n2\n25\n80\n65\n\n\n5\n6\n2\n50\n89\n98\n\n\n6\n7\n2\n80\n90\n45\n\n\n7\n8\n2\n90\n78\n25\n\n\n8\n9\n3\n20\n98\n15\n\n\n9\n10\n3\n50\n98\n45\n\n\n10\n11\n3\n65\n65\n65\n\n\n11\n12\n3\n45\n85\n32\n\n\n12\n13\n4\n46\n98\n65\n\n\n13\n14\n4\n48\n87\n12\n\n\n14\n15\n4\n75\n56\n78\n\n\n15\n16\n4\n58\n98\n65\n\n\n16\n17\n5\n65\n68\n98\n\n\n17\n18\n5\n80\n78\n90\n\n\n18\n19\n5\n89\n68\n87\n\n\n19\n20\n5\n78\n83\n58\n\n\n\n\n\n\n\n학급별 수학 성적 분포 산점도\n\npx.scatter(data_frame=exam,\n            x='nclass', y='math', color='nclass')\n\n                                                \n\n\n학급별 수학 성적 평균 막대그래프\n\ndf=exam.groupby('nclass', as_index=False).agg(math_mean=('math','mean'))\ndf\npx.bar(data_frame=df, x='nclass', y='math_mean', color='nclass')\n\n                                                \n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw2.1/index.html#plotly를-활용한-인터랙티브-시각화",
    "href": "posts/hw2.1/index.html#plotly를-활용한-인터랙티브-시각화",
    "title": "HW2.1",
    "section": "",
    "text": "exam데이터 불러오기\n\nimport pandas as pd\nimport plotly.express as px\nexam=pd.read_csv('../../data/exam.csv')\nexam\n\n\n\n\n\n\n\n\nid\nnclass\nmath\nenglish\nscience\n\n\n\n\n0\n1\n1\n50\n98\n50\n\n\n1\n2\n1\n60\n97\n60\n\n\n2\n3\n1\n45\n86\n78\n\n\n3\n4\n1\n30\n98\n58\n\n\n4\n5\n2\n25\n80\n65\n\n\n5\n6\n2\n50\n89\n98\n\n\n6\n7\n2\n80\n90\n45\n\n\n7\n8\n2\n90\n78\n25\n\n\n8\n9\n3\n20\n98\n15\n\n\n9\n10\n3\n50\n98\n45\n\n\n10\n11\n3\n65\n65\n65\n\n\n11\n12\n3\n45\n85\n32\n\n\n12\n13\n4\n46\n98\n65\n\n\n13\n14\n4\n48\n87\n12\n\n\n14\n15\n4\n75\n56\n78\n\n\n15\n16\n4\n58\n98\n65\n\n\n16\n17\n5\n65\n68\n98\n\n\n17\n18\n5\n80\n78\n90\n\n\n18\n19\n5\n89\n68\n87\n\n\n19\n20\n5\n78\n83\n58\n\n\n\n\n\n\n\n학급별 수학 성적 분포 산점도\n\npx.scatter(data_frame=exam,\n            x='nclass', y='math', color='nclass')\n\n                                                \n\n\n학급별 수학 성적 평균 막대그래프\n\ndf=exam.groupby('nclass', as_index=False).agg(math_mean=('math','mean'))\ndf\npx.bar(data_frame=df, x='nclass', y='math_mean', color='nclass')\n\n                                                \n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "HW3",
    "section": "",
    "text": "문제 1\n정규분포 pdf 값을 계산하는 자신만의 파이썬 함수를 정의하고, 정규분포 mu = 3, sigma = 2 의 pdf를 그릴 것.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport math\n\ndef Norm(x, mu, sigma):\n    return (1/(sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)\nk=np.linspace(-7,13,100)\n\nmy_n=Norm(k,3,2)\n\nplt.plot(k,my_n)\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n문제 2\n파이썬 scipy 패키지 사용해서 다음과 같은 확률을 구하시오.\n\nX ~ N(2, 3^2)\n\nP(X &lt; 3)\n\n\n\nnorm.cdf(3, loc=2, scale=3)\n\n0.6305586598182363\n\n\n\n\nP(2 &lt; X &lt; 5)\n\n\n\nnorm.cdf(5, loc=2, scale=3)-norm.cdf(2, loc=2, scale=3)\n\n0.3413447460685429\n\n\n\n\nP(X &lt; 3 or X &gt; 7)\n\n\n\nnorm.cdf(3, loc=2, scale=3)+1-norm.cdf(7, loc=2, scale=3)\n\n0.6783490120910509\n\n\n\n\n문제 3\nLS 빅데이터 스쿨 학생들의 중간고사 점수는 평균이 30이고, 분산이 4인 정규분포를 따른다. 상위 5%에 해당하는 학생의 점수는?\n\nscore=norm.ppf(0.95, loc=30, scale=2)\nprint(score)\n\n33.28970725390295\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw5/index.html",
    "href": "posts/hw5/index.html",
    "title": "HW5",
    "section": "",
    "text": "성별에 따른 월급 설문조사 그래프에서 각 성별 95% 신뢰구간 계산 후 그리기 norm.ppf() 사용해서 그릴 것. 모분산은 표본분산을 사용해서 규정.\n\n기존 그래프\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nraw_welfare = pd.read_spss('../../data/Koweps_hpwc14_2019_beta2.sav')\nwelfare=raw_welfare.copy()\n\nwelfare = welfare.rename(\n    columns = {'h14_g3' : 'sex',\n               'h14_g4' : 'birth',\n               'h14_g10' : 'marriage_type',\n               'h14_g11' : 'religion',\n               'p1402_8aq1' : 'income',\n               'h14_eco9' : 'code_job',\n               'h14_reg7' : 'code_religion',}\n)\n\nwelfare = welfare[['sex','birth', 'marriage_type', 'religion',\n                   'income', 'code_job', 'code_religion']]\n                   \nwelfare['sex']=np.where(welfare['sex']==1, 'male', 'female')\n\nsex_income = welfare.dropna(subset = 'income')\\\n                    .groupby('sex', as_index = False)\\\n                    .agg(mean_income = ('income', 'mean'))\n                    \nsns.barplot(data = sex_income, x = 'sex', y = 'mean_income', hue = 'sex')\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n표본평균과 표본분산 구하기\n\nmean_male = sex_income.iloc[1,1]\nmean_female = sex_income.iloc[0,1]\nprint('mean male: ', mean_male)\n\nmean male:  349.03757099169945\n\n\n\nprint('mean female: ', mean_female)\n\nmean female:  186.29309576837417\n\n\n\nstd_income = welfare.dropna(subset = 'income')\\\n                    .groupby('sex', as_index = False)\\\n                    .agg(std_income = ('income', 'std'))\nstd_male = std_income.iloc[1,1]\nstd_female = std_income.iloc[0,1]\n\nprint('std. male: ', std_male)\n\nstd. male:  217.86225435930135\n\n\n\nprint('std. female:', std_female)\n\nstd. female: 132.05740180730666\n\n\n\n\n신뢰구간 구하기\n\nfrom scipy.stats import norm\nz_0025 = norm.ppf(0.975, loc=0, scale=1)\nn_male = welfare.dropna(subset = 'income').query(\"sex=='male'\")['income'].count()\nn_female = welfare.dropna(subset = 'income').query(\"sex=='female'\")['income'].count()\nci_male1 = mean_male + z_0025 * std_male/np.sqrt(n_male)\nci_male2 = mean_male - z_0025 * std_male/np.sqrt(n_male)\nci_female1 = mean_female + z_0025 * std_female/np.sqrt(n_female)\nci_female2 = mean_female - z_0025 * std_female/np.sqrt(n_female)\nci_male1\nci_male2\n\nprint('c.i.male upper: ', ci_male1)\n\nc.i.male upper:  357.96254968365116\n\n\n\nprint('c.i.male lower: ', ci_male2)\n\nc.i.male lower:  340.11259229974775\n\n\n\nprint('c.i.female upper: ', ci_female1)\n\nc.i.female upper:  191.7557368532799\n\n\n\nprint('c.i.female lower: ', ci_female1)\n\nc.i.female lower:  191.7557368532799\n\n\n\n\n그래프 그리기\n\nsns.barplot(data = sex_income, x = 'sex', y = 'mean_income', hue = 'sex')\nplt.plot([0,0], [ci_female1 , ci_female2], color='r')\nplt.plot([1,1], [ci_male1 , ci_male2], color='r')\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/hw7/index.html",
    "href": "posts/hw7/index.html",
    "title": "HW7",
    "section": "",
    "text": "2022년에 실시된 ADP실기 시험의 통계파트 표준점수는 평균이 30, 표준편차가 5인 정규분포를 따른다고 한다.\n\nADP 실기 시험의 통계파트 표준점수의 밀도함수를 그려보세요\n\n\nfrom scipy.stats import norm\nimport numpy as np\nimport matplotlib.pyplot as plt\nk = np.linspace(10, 50, 100)\ny = norm.pdf(k, loc=30, scale=5)\nplt.plot(k, y)\n\n\n\n\n\n\n\n\n\nADP 수험생을 임의로 1명을 선택하여 통계 점수를 조회했을때 45점 보다 높은 점수를 받았을 확률을 구하세요.\n\n\np = 1-norm.cdf(45, loc=30, scale=5)\nprint(round(p, 6))\n\n0.00135\n\n\n\n슬통이는 상위 10%에 해당하는 점수를 얻었다고 한다면, 슬통이의 점수는 얼마인지 계산해보세요.\n\n\nscore = norm.ppf(0.9, loc=30, scale=5)\nprint(round(score, 3))\n\n36.408\n\n\n\n슬기로운 통계생활의 해당 회차 수강생은 16명이었다고 한다. 16명의 통계 파트 점수를 평균내었을 때, 이 평균값이 따르는 분포의 확률밀도 함수를 1번의 그래프와 겹쳐 그려보세요.\n\n\nk = np.linspace(10, 50, 100)\ny = norm.pdf(k, loc=30, scale=5)\nplt.plot(k, y)\n\nX_bar = norm.pdf(k, loc=30, scale=5/np.sqrt(16))\nplt.plot(k, X_bar)\n\n\n\n\n\n\n\n\n-슬기로운 통계생활 ADP 반 수강생들의 통계점수를 평균내었다고 할 때, 이 값이 38점보다 높게 나올 확률을 구하세요.\n\np = 1-norm.cdf(38, loc=30, scale=5/np.sqrt(16))\nprint(p)\n\n7.76885222819601e-11"
  },
  {
    "objectID": "posts/hw7/index.html#adp-표본점수",
    "href": "posts/hw7/index.html#adp-표본점수",
    "title": "HW7",
    "section": "",
    "text": "2022년에 실시된 ADP실기 시험의 통계파트 표준점수는 평균이 30, 표준편차가 5인 정규분포를 따른다고 한다.\n\nADP 실기 시험의 통계파트 표준점수의 밀도함수를 그려보세요\n\n\nfrom scipy.stats import norm\nimport numpy as np\nimport matplotlib.pyplot as plt\nk = np.linspace(10, 50, 100)\ny = norm.pdf(k, loc=30, scale=5)\nplt.plot(k, y)\n\n\n\n\n\n\n\n\n\nADP 수험생을 임의로 1명을 선택하여 통계 점수를 조회했을때 45점 보다 높은 점수를 받았을 확률을 구하세요.\n\n\np = 1-norm.cdf(45, loc=30, scale=5)\nprint(round(p, 6))\n\n0.00135\n\n\n\n슬통이는 상위 10%에 해당하는 점수를 얻었다고 한다면, 슬통이의 점수는 얼마인지 계산해보세요.\n\n\nscore = norm.ppf(0.9, loc=30, scale=5)\nprint(round(score, 3))\n\n36.408\n\n\n\n슬기로운 통계생활의 해당 회차 수강생은 16명이었다고 한다. 16명의 통계 파트 점수를 평균내었을 때, 이 평균값이 따르는 분포의 확률밀도 함수를 1번의 그래프와 겹쳐 그려보세요.\n\n\nk = np.linspace(10, 50, 100)\ny = norm.pdf(k, loc=30, scale=5)\nplt.plot(k, y)\n\nX_bar = norm.pdf(k, loc=30, scale=5/np.sqrt(16))\nplt.plot(k, X_bar)\n\n\n\n\n\n\n\n\n-슬기로운 통계생활 ADP 반 수강생들의 통계점수를 평균내었다고 할 때, 이 값이 38점보다 높게 나올 확률을 구하세요.\n\np = 1-norm.cdf(38, loc=30, scale=5/np.sqrt(16))\nprint(p)\n\n7.76885222819601e-11"
  },
  {
    "objectID": "posts/hw7/index.html#covid-19-발병률",
    "href": "posts/hw7/index.html#covid-19-발병률",
    "title": "HW7",
    "section": "Covid 19 발병률",
    "text": "Covid 19 발병률\nCovid‑19의 발병률은 1%라고 한다. 다음은 이번 코로나 사태로 인하여 코로나 의심 환자들 1,085명을 대상으로 슬통 회사의 “다잡아” 키트를 사용하여 양성 반응을 체크한 결과이다.\n\nimport pandas as pd\npd.DataFrame({\n    \"키트\\실제\": [\"양성\", \"음성\"],\n    \"양성\": [370, 15],\n    \"음성\": [10, 690]\n})\n\n\n\n\n\n\n\n\n키트\\실제\n양성\n음성\n\n\n\n\n0\n양성\n370\n10\n\n\n1\n음성\n15\n690\n\n\n\n\n\n\n\n\n다잡아 키트가 코로나 바이러스에 걸린 사람을 양성으로 잡아낼 확률을 계산하세요.\n\n\n# P(키트 양성|실제 양성)\np = 370 / (370 + 15)\nprint(round(p, 3))\n\n0.961\n\n\n\n슬통 회사에서 다잡아 키트를 사용해 양성으로 나온 사람이 실제로는 코로나 바이러스에 걸려있을 확률을 97%라며, 키트의 우수성을 주장했다. 이 주장이 옳지 않은 이유를 서술하세요.\n\n표본집단의 확률인 0.961038961038961과 차이가 큼\n\n\nCovid‑19 발병률을 사용하여, 키트의 결과값이 양성으로 나온 사람이 실제로 코로나 바이러스에 걸려있을 확률을 구하세요.\n\n\\[\n\\begin{aligned}\nP(\\text{실제 양성} \\mid \\text{키트 양성}) &= \\frac{P(\\text{키트 양성} \\cap \\text{실제 양성})}{P(\\text{키트 양성})} \\\\\n&= \\frac{P(\\text{실제 양성}) \\cdot P(\\text{키트 양성} \\mid \\text{실제 양성})}\n{P(\\text{실제 양성}) \\cdot P(\\text{키트 양성} \\mid \\text{실제 양성}) + P(\\text{실제 음성}) \\cdot P(\\text{키트 양성} \\mid \\text{실제 음성})}\n\\end{aligned}\n\\]\n\np = 0.01*(370/385) / (0.01*(370/385)+0.99*(10/700))\nprint(round(p, 3))\n\n0.405"
  },
  {
    "objectID": "posts/hw7/index.html#카이제곱분포와-표본분산",
    "href": "posts/hw7/index.html#카이제곱분포와-표본분산",
    "title": "HW7",
    "section": "카이제곱분포와 표본분산",
    "text": "카이제곱분포와 표본분산\n\n자유도가 4인 카이제곱분포의 확률밀도함수를 그려보세요.\n\n\nfrom scipy.stats import chi2\nk = np.linspace(0, 20, 100)\ny = chi2.pdf(k, df=4)\nplt.plot(k, y)\n\n\n\n\n\n\n\n\n\n다음의 확률을 구해보세요. 𝑃 (3 ≤ 𝑋 ≤ 5)\n\n\np = chi2.cdf(5, df=4) - chi2.cdf(3, df=4)\nprint(round(p, 3))\n\n0.271\n\n\n\n자유도가 4인 카이제곱분포에서 크기가 1000인 표본을 뽑은 후, 히스토그램을 그려보세요.\n\n\nimport seaborn as sns\nnp.random.seed(20240902)\nx=chi2.rvs(size=1000, df=4)\nsns.histplot(x)\n\n\n\n\n\n\n\n\n\n자유도가 4인 카이제곱분포를 따르는 확률변수에서 나올 수 있는 값 중 상위 5%에 해당하는 값은 얼마인지 계산해보세요.\n\n\nx = chi2.ppf(0.95, df=4)\nprint(round(x, 3))\n\n9.488\n\n\n\n3번에서 뽑힌 표본값들 중 상위 5%에 위치한 표본의 값은 얼마인가요?\n\n\nx = np.percentile(x, 95)\nprint(round(x, 3))\n\n9.488\n\n\n\n평균이 3, 표준편차가 2인 정규분포를 따르는 확률변수에서 크기가 20인 표본, x_1, …, x_20,을 뽑은 후 표본분산을 계산한 것을 s^2_1이라 생각해보죠. 다음을 수행해보세요!\n\n\n# 같은 방법으로 500개의 s^2 들, s^2_1,s^2_2,...,s^2_500 발생시킵니다.\nvar=[]\nfor i in range(500):\n    x=norm.rvs(size=20, loc=3, scale=2)\n    var.append(np.var(x, ddof=1))\n\n\n# 발생한 500개의 s^2 들 각각에 4.75를 곱하고, 그것들의 히스토그램을 그려보세요. (히스토그램을 그릴 때 probability = TRUE 옵션을 사용해서 그릴 것)\na=np.array(var)*4.75\nsns.histplot(a, stat='density')\n# 위에서 그린 히스토그램에 자유도가 19인 카이제곱분포 확률밀도함수를 겹쳐그려보세요.\nk = np.linspace(a.min(), a.max(), 100)\ny = chi2.pdf(k, df=19)\nplt.plot(k, y, color='red')\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nKaggle: Houseprice -stacking\n\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle: Houseprice - Tensorflow\n\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nYOLO8\n\n\n\nSep 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW7.1\n\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW7\n\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAmes 특징 및 교육 대쉬보드\n\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW6\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n계절별 화재발생 빈도 및 요인 분석\n\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW5\n\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW4\n\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW3\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2.2\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2.1\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\nJul 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHW1\n\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]